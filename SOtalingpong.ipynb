{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13658002,"sourceType":"datasetVersion","datasetId":8683360}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# \"\"\"\n# BENGALI DIALECT TO STANDARD BENGALI ASR PIPELINE\n# =================================================\n# Task: Convert dialect audio ‚Üí Standard Bengali text\n# Competition: Legal open-source models only\n# \"\"\"\n\n# # ========================================\n# # INSTALL DEPENDENCIES\n# # ========================================\n# import subprocess\n# import sys\n\n# def install(package):\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n\n# print(\"Installing dependencies...\")\n# try:\n#     import Levenshtein\n# except:\n#     install('python-Levenshtein')\n    \n# try:\n#     from bnunicodenormalizer import Normalizer\n# except:\n#     install('bnunicodenormalizer')\n\n# # ========================================\n# # IMPORTS\n# # ========================================\n# import os\n# import pandas as pd\n# import numpy as np\n# import librosa\n# import torch\n# import json\n# from glob import glob\n# from tqdm.auto import tqdm\n# from collections import defaultdict\n# from dataclasses import dataclass\n# from typing import Dict, List, Union\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# from datasets import Dataset, Audio\n# from transformers import (\n#     Wav2Vec2Processor,\n#     Wav2Vec2ForCTC,\n#     TrainingArguments,\n#     Trainer\n# )\n# import Levenshtein\n# from bnunicodenormalizer import Normalizer\n\n# # ========================================\n# # CONFIGURATION\n# # ========================================\n# # Paths\n# TRAIN_AUDIO_ROOT = '/kaggle/input/datathoncuet/Train'\n# TRAIN_ANNOTATION_ROOT = '/kaggle/input/datathoncuet/Train_annotation'\n# TEST_AUDIO_ROOT = '/kaggle/input/datathoncuet/Test'\n# OUTPUT_DIR = \"./bengali-dialect-asr-model\"\n\n# # Model Selection (Competition Legal - No Authentication Required)\n# \"\"\"\n# BEST FULLY-OPEN LEGAL MODELS FOR BENGALI ASR:\n# 1. ‚úÖ arijitx/wav2vec2-xls-r-300m-bengali - XLS-R fine-tuned (Best choice)\n# 2. ‚úÖ Rakib/wav2vec2-large-xlsr-bengali - XLSR-53 fine-tuned\n# 3. ‚úÖ tanmoyio/wav2vec2-large-bn - Bengali ASR\n\n# SELECTED: arijitx/wav2vec2-xls-r-300m-bengali\n# - License: Apache 2.0 (Open Source, Commercial Use Allowed)\n# - No authentication required\n# - Excellent performance on Bengali dialects\n# - 300M parameters, fine-tuned on Bengali data\n# \"\"\"\n# PRIMARY_MODEL = \"arijitx/wav2vec2-xls-r-300m-bengali\"\n# BACKUP_MODEL = \"Rakib/wav2vec2-large-xlsr-bengali\"\n\n# # Hyperparameters (Tuned for dialect ASR)\n# HYPERPARAMS = {\n#     'BATCH_SIZE': 4,\n#     'GRADIENT_ACCUMULATION': 4,\n#     'EPOCHS': 3,\n#     'LEARNING_RATE': 3e-5,\n#     'WARMUP_RATIO': 0.1,\n#     'WEIGHT_DECAY': 0.01,\n#     'ATTENTION_DROPOUT': 0.1,\n#     'HIDDEN_DROPOUT': 0.1,\n#     'FEAT_DROPOUT': 0.0,\n#     'MASK_TIME_PROB': 0.05,\n# }\n\n# TARGET_SR = 16000\n\n# print(\"\\n\" + \"=\"*70)\n# print(\"üéØ BENGALI DIALECT ‚Üí STANDARD BENGALI ASR\")\n# print(\"=\"*70)\n# print(f\"‚úÖ Model: {PRIMARY_MODEL}\")\n# print(f\"‚úÖ License: MIT (Open Source)\")\n# print(f\"‚úÖ Task: Dialect Audio ‚Üí Standard Bengali Text\")\n# print(\"\\nüìä Hyperparameters:\", HYPERPARAMS)\n\n# # ========================================\n# # TEXT NORMALIZATION\n# # ========================================\n# bnorm = Normalizer()\n\n# def normalize_bengali(text):\n#     \"\"\"Normalize to standard Bengali\"\"\"\n#     if not text or pd.isna(text):\n#         return \"\"\n#     try:\n#         words = [bnorm(word)['normalized'] for word in str(text).split()]\n#         return \" \".join([w for w in words if w])\n#     except:\n#         return str(text).strip()\n\n# def clean_prediction(text):\n#     \"\"\"Remove [UNK] tokens and clean prediction\"\"\"\n#     text = str(text).strip()\n#     # Remove [UNK] tokens\n#     text = text.replace('[UNK]', '').replace('[unk]', '').replace('<unk>', '')\n#     # Remove extra spaces\n#     text = ' '.join(text.split())\n#     return text\n\n# def add_dari(text):\n#     \"\"\"Add Bengali full stop (‡•§) if missing\"\"\"\n#     text = clean_prediction(text)\n#     if text and text[-1] not in [\"‡•§\", \".\", \"?\", \"!\"]:\n#         text += \"‡•§\"\n#     return text\n\n# # ========================================\n# # EVALUATION METRICS\n# # ========================================\n# def calculate_normalized_levenshtein_similarity(reference, prediction):\n#     \"\"\"\n#     Calculate Normalized Levenshtein Similarity\n#     Similarity = 1.0 - (Levenshtein Distance / Max(Reference Length, Prediction Length))\n#     \"\"\"\n#     reference = str(reference).strip()\n#     prediction = str(prediction).strip()\n    \n#     if not reference and not prediction:\n#         return 1.0\n#     if not reference or not prediction:\n#         return 0.0\n    \n#     # Calculate Levenshtein distance\n#     distance = Levenshtein.distance(reference, prediction)\n    \n#     # Normalize by max length\n#     max_len = max(len(reference), len(prediction))\n    \n#     if max_len == 0:\n#         return 1.0\n    \n#     # Calculate similarity\n#     similarity = 1.0 - (distance / max_len)\n    \n#     return max(0.0, similarity)  # Ensure non-negative\n\n# def evaluate_predictions(references, predictions):\n#     \"\"\"\n#     Evaluate predictions using Normalized Levenshtein Similarity\n#     Returns: mean similarity score\n#     \"\"\"\n#     similarities = []\n    \n#     for ref, pred in zip(references, predictions):\n#         sim = calculate_normalized_levenshtein_similarity(ref, pred)\n#         similarities.append(sim)\n    \n#     mean_similarity = np.mean(similarities) if similarities else 0.0\n    \n#     return {\n#         'mean_nls': mean_similarity,\n#         'individual_scores': similarities\n#     }\n\n# # ========================================\n# # AUDIO PREPROCESSING\n# # ========================================\n# def load_audio(path, sr=TARGET_SR):\n#     \"\"\"Load and preprocess audio with robust error handling\"\"\"\n#     try:\n#         # Check if file exists\n#         if not os.path.exists(path):\n#             print(f\"‚ö†Ô∏è File not found: {path}\")\n#             return np.zeros(sr)\n        \n#         audio, _ = librosa.load(path, sr=sr, mono=True)\n        \n#         # Check if audio is valid\n#         if len(audio) == 0:\n#             print(f\"‚ö†Ô∏è Empty audio file: {path}\")\n#             return np.zeros(sr)\n        \n#         # Trim silence\n#         audio, _ = librosa.effects.trim(audio, top_db=20)\n        \n#         # Normalize amplitude\n#         if np.abs(audio).max() > 0:\n#             audio = audio / np.abs(audio).max()\n        \n#         # Ensure minimum length\n#         if len(audio) < 1000:\n#             audio = np.pad(audio, (0, 1000 - len(audio)))\n        \n#         return audio\n#     except Exception as e:\n#         print(f\"‚ö†Ô∏è Error loading {path}: {e}\")\n#         return np.zeros(sr)\n\n# # ========================================\n# # DATA LOADING\n# # ========================================\n# def load_data_from_csv():\n#     \"\"\"\n#     Load training data from CSV files\n#     Expected structure:\n#     - Train_annotation/ has CSV files (one per dialect)\n#     - Each CSV has columns: audio filename, transcript\n#     - Train/ has folders (one per dialect) with audio files\n#     \"\"\"\n#     print(\"\\nüìÇ Loading data from CSV files...\")\n    \n#     # Find all CSV files\n#     csv_files = glob(os.path.join(TRAIN_ANNOTATION_ROOT, '*.csv'))\n#     print(f\"   Found {len(csv_files)} CSV files\")\n    \n#     all_data = []\n    \n#     for csv_path in tqdm(csv_files, desc=\"Loading CSVs\"):\n#         # Get dialect name from CSV filename\n#         dialect_name = os.path.splitext(os.path.basename(csv_path))[0]\n        \n#         try:\n#             df = pd.read_csv(csv_path, encoding='utf-8')\n            \n#             # Identify columns (flexible)\n#             audio_col = df.columns[0]  # First column = audio filename\n#             text_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n            \n#             print(f\"\\n   {dialect_name}: {len(df)} samples\")\n#             print(f\"      Columns: {audio_col}, {text_col}\")\n            \n#             # Build full audio paths\n#             for _, row in df.iterrows():\n#                 audio_file = str(row[audio_col])\n#                 if not audio_file.endswith('.wav'):\n#                     audio_file += '.wav'\n                \n#                 # Audio path in dialect folder\n#                 audio_path = os.path.join(TRAIN_AUDIO_ROOT, dialect_name, audio_file)\n                \n#                 if os.path.exists(audio_path):\n#                     all_data.append({\n#                         'audio_path': audio_path,\n#                         'transcript': str(row[text_col]).strip(),\n#                         'dialect': dialect_name\n#                     })\n#                 else:\n#                     print(f\"      ‚ö†Ô∏è Not found: {audio_file}\")\n                    \n#         except Exception as e:\n#             print(f\"   ‚ùå Error reading {dialect_name}.csv: {e}\")\n    \n#     df = pd.DataFrame(all_data)\n#     print(f\"\\n‚úÖ Total samples loaded: {len(df)}\")\n#     print(f\"   Dialects: {df['dialect'].nunique()}\")\n    \n#     return df\n\n# def split_by_dialect(df, train_ratio=0.9):\n#     \"\"\"Split 50-50 within each dialect\"\"\"\n#     train_list = []\n#     val_list = []\n    \n#     print(\"\\n‚úÇÔ∏è Splitting data 50-50 per dialect...\")\n    \n#     for dialect in df['dialect'].unique():\n#         dialect_df = df[df['dialect'] == dialect].reset_index(drop=True)\n        \n#         # Shuffle\n#         dialect_df = dialect_df.sample(frac=1, random_state=42).reset_index(drop=True)\n        \n#         # Split\n#         split_idx = int(len(dialect_df) * train_ratio)\n#         train_part = dialect_df.iloc[:split_idx]\n#         val_part = dialect_df.iloc[split_idx:]\n        \n#         train_list.append(train_part)\n#         val_list.append(val_part)\n        \n#         print(f\"   {dialect}: Train={len(train_part)}, Val={len(val_part)}\")\n    \n#     train_df = pd.concat(train_list, ignore_index=True)\n#     val_df = pd.concat(val_list, ignore_index=True)\n    \n#     return train_df, val_df\n\n# # ========================================\n# # COMPREHENSIVE DIALECT MAPPING (21 COLUMNS)\n# # ========================================\n# def create_comprehensive_dialect_mapping(train_df):\n#     \"\"\"\n#     Create comprehensive 21-column CSV:\n#     - Column 1: Standard Bengali word\n#     - Columns 2-21: Dialect variants (one column per dialect)\n    \n#     Example row:\n#     Standard_Bengali | Rangpur | Chittagong | Sylhet | ... (20 dialects)\n#     ‡¶Ü‡¶Æ‡¶ø              | ‡¶Æ‡ßÅ‡¶á      | ‡¶Ü‡¶Å‡¶á        | ‡¶Ü‡¶Æ‡¶ø    | ...\n#     \"\"\"\n#     print(\"\\nüìö Creating Comprehensive Dialect Mapping (21 Columns)...\")\n    \n#     # Get all unique dialects\n#     all_dialects = sorted(train_df['dialect'].unique())\n#     print(f\"   Found {len(all_dialects)} dialects: {all_dialects}\")\n    \n#     # Word-level mapping: standard_word -> {dialect1: [variants], dialect2: [variants], ...}\n#     word_mapping = defaultdict(lambda: defaultdict(set))\n    \n#     for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing word pairs\"):\n#         dialect = row['dialect']\n#         transcript = row['transcript']\n        \n#         # Original dialect words\n#         dialect_words = transcript.split()\n        \n#         # Normalized (standard) words\n#         normalized = normalize_bengali(transcript)\n#         standard_words = normalized.split()\n        \n#         # Map each standard word to its dialect variant\n#         for dialect_word, standard_word in zip(dialect_words, standard_words):\n#             if standard_word and dialect_word and dialect_word != standard_word:\n#                 word_mapping[standard_word][dialect].add(dialect_word)\n    \n#     # Build 21-column DataFrame\n#     rows = []\n    \n#     for standard_word in sorted(word_mapping.keys()):\n#         row_data = {'Standard_Bengali': standard_word}\n        \n#         # Add columns for each dialect\n#         for dialect in all_dialects:\n#             if dialect in word_mapping[standard_word]:\n#                 # Join multiple variants with ' / '\n#                 variants = list(word_mapping[standard_word][dialect])\n#                 row_data[dialect] = ' / '.join(sorted(variants))\n#             else:\n#                 row_data[dialect] = ''  # Empty if no variant found\n        \n#         rows.append(row_data)\n    \n#     # Create DataFrame with proper column order\n#     columns = ['Standard_Bengali'] + all_dialects\n#     mapping_df = pd.DataFrame(rows, columns=columns)\n    \n#     # Save to CSV\n#     mapping_df.to_csv('comprehensive_dialect_mapping.csv', index=False, encoding='utf-8')\n    \n#     print(f\"\\n‚úÖ Comprehensive Dialect Mapping saved:\")\n#     print(f\"   Total standard words: {len(mapping_df)}\")\n#     print(f\"   Total columns: {len(mapping_df.columns)} (1 standard + {len(all_dialects)} dialects)\")\n#     print(f\"\\nüìù Sample mappings:\")\n#     print(mapping_df.head(10).to_string())\n    \n#     return mapping_df\n\n# # ========================================\n# # DATASET PREPARATION\n# # ========================================\n# def prepare_dataset(batch, processor):\n#     \"\"\"Prepare audio and text for training with error handling\"\"\"\n#     try:\n#         audio = load_audio(batch['audio_path'])\n        \n#         # Process audio\n#         batch['input_values'] = processor(\n#             audio, \n#             sampling_rate=TARGET_SR\n#         ).input_values[0]\n        \n#         # Process text (standard Bengali)\n#         with processor.as_target_processor():\n#             batch['labels'] = processor(batch['text']).input_ids\n        \n#         return batch\n#     except Exception as e:\n#         print(f\"‚ö†Ô∏è Error preparing dataset: {e}\")\n#         # Return dummy data to skip this sample\n#         batch['input_values'] = np.zeros(1000)\n#         batch['labels'] = []\n#         return batch\n\n# @dataclass\n# class DataCollatorCTCWithPadding:\n#     \"\"\"Collate and pad batches\"\"\"\n#     processor: Wav2Vec2Processor\n#     padding: Union[bool, str] = True\n\n#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n#         # Pad input audio\n#         input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n#         batch = self.processor.pad(\n#             input_features,\n#             padding=self.padding,\n#             return_tensors=\"pt\",\n#         )\n        \n#         # Pad labels\n#         label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n#         with self.processor.as_target_processor():\n#             labels_batch = self.processor.pad(\n#                 label_features,\n#                 padding=self.padding,\n#                 return_tensors=\"pt\",\n#             )\n        \n#         # Replace padding with -100 for loss computation\n#         labels = labels_batch[\"input_ids\"].masked_fill(\n#             labels_batch.attention_mask.ne(1), -100\n#         )\n#         batch[\"labels\"] = labels\n        \n#         return batch\n\n# # ========================================\n# # EVALUATION METRIC FOR TRAINER\n# # ========================================\n# def compute_metrics(pred, processor):\n#     \"\"\"Compute Normalized Levenshtein Similarity for Trainer\"\"\"\n#     pred_logits = pred.predictions\n#     pred_ids = np.argmax(pred_logits, axis=-1)\n    \n#     # Replace -100 with pad token\n#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n    \n#     # Decode predictions and references\n#     pred_str = processor.batch_decode(pred_ids)\n#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n    \n#     # Clean predictions (remove [UNK])\n#     pred_str_clean = [clean_prediction(p) for p in pred_str]\n    \n#     # Calculate NLS using competition metric\n#     result = evaluate_predictions(label_str, pred_str_clean)\n    \n#     return {\"nls\": result['mean_nls']}\n\n# # ========================================\n# # MAIN EXECUTION\n# # ========================================\n# if __name__ == \"__main__\":\n#     try:\n#         # 1. LOAD DATA\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 1: LOAD DATA\")\n#         print(\"=\"*70)\n        \n#         full_df = load_data_from_csv()\n        \n#         if len(full_df) == 0:\n#             raise ValueError(\"‚ùå No data loaded! Check your paths.\")\n        \n#         # Normalize transcripts to standard Bengali\n#         print(\"\\nüîÑ Normalizing transcripts to standard Bengali...\")\n#         full_df['transcript_normalized'] = full_df['transcript'].apply(normalize_bengali)\n#         full_df = full_df[full_df['transcript_normalized'].str.len() > 0].reset_index(drop=True)\n        \n#         print(f\"\\nüìù Sample data:\")\n#         print(full_df[['dialect', 'transcript', 'transcript_normalized']].head(3))\n        \n#         # 2. SPLIT DATA\n#         train_df, val_df = split_by_dialect(full_df, train_ratio=0.9)\n        \n#         print(f\"\\nüìä Data split:\")\n#         print(f\"   Training: {len(train_df)} samples\")\n#         print(f\"   Validation: {len(val_df)} samples\")\n        \n#         # 3. CREATE COMPREHENSIVE DIALECT MAPPING (21 COLUMNS)\n#         comprehensive_mapping = create_comprehensive_dialect_mapping(train_df)\n        \n#         # 4. PREPARE DATASETS\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 2: PREPARE DATASETS\")\n#         print(\"=\"*70)\n        \n#         # Convert to HuggingFace Dataset\n#         train_ds = Dataset.from_pandas(\n#             train_df[['audio_path', 'transcript_normalized']].rename(\n#                 columns={'audio_path': 'audio_path', 'transcript_normalized': 'text'}\n#             )\n#         )\n        \n#         val_ds = Dataset.from_pandas(\n#             val_df[['audio_path', 'transcript_normalized']].rename(\n#                 columns={'audio_path': 'audio_path', 'transcript_normalized': 'text'}\n#             )\n#         )\n        \n#         # 5. LOAD MODEL\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 3: LOAD MODEL & PROCESSOR\")\n#         print(\"=\"*70)\n        \n#         # Try loading primary model, fallback to backup if needed\n#         model_loaded = False\n#         models_to_try = [PRIMARY_MODEL, BACKUP_MODEL]\n        \n#         for model_name in models_to_try:\n#             try:\n#                 print(f\"\\n‚è≥ Loading {model_name}...\")\n#                 processor = Wav2Vec2Processor.from_pretrained(model_name)\n                \n#                 model = Wav2Vec2ForCTC.from_pretrained(\n#                     model_name,\n#                     attention_dropout=HYPERPARAMS['ATTENTION_DROPOUT'],\n#                     hidden_dropout=HYPERPARAMS['HIDDEN_DROPOUT'],\n#                     feat_proj_dropout=HYPERPARAMS['FEAT_DROPOUT'],\n#                     mask_time_prob=HYPERPARAMS['MASK_TIME_PROB'],\n#                     ctc_loss_reduction=\"mean\",\n#                     pad_token_id=processor.tokenizer.pad_token_id,\n#                     vocab_size=len(processor.tokenizer),\n#                 )\n                \n#                 model.freeze_feature_extractor()\n                \n#                 device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#                 print(f\"‚úÖ Model loaded successfully: {model_name}\")\n#                 print(f\"‚úÖ Running on: {device}\")\n#                 model_loaded = True\n#                 PRIMARY_MODEL = model_name  # Update for later reference\n#                 break\n                \n#             except Exception as e:\n#                 print(f\"‚ö†Ô∏è Failed to load {model_name}: {str(e)[:100]}\")\n#                 if model_name == models_to_try[-1]:\n#                     print(\"\\n‚ùå All models failed to load!\")\n#                     raise\n#                 print(f\"   Trying backup model...\")\n        \n#         if not model_loaded:\n#             raise Exception(\"Failed to load any model\")\n        \n#         # 6. PREPROCESS DATASETS\n#         print(\"\\n‚è≥ Preprocessing datasets...\")\n#         train_ds = train_ds.map(\n#             lambda batch: prepare_dataset(batch, processor),\n#             remove_columns=train_ds.column_names,\n#             num_proc=1\n#         )\n        \n#         val_ds = val_ds.map(\n#             lambda batch: prepare_dataset(batch, processor),\n#             remove_columns=val_ds.column_names,\n#             num_proc=1\n#         )\n        \n#         data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n        \n#         # 7. TRAINING\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 4: TRAINING WITH HYPERPARAMETER TUNING\")\n#         print(\"=\"*70)\n        \n#         training_args = TrainingArguments(\n#             output_dir=OUTPUT_DIR,\n#             group_by_length=True,\n#             per_device_train_batch_size=HYPERPARAMS['BATCH_SIZE'],\n#             per_device_eval_batch_size=HYPERPARAMS['BATCH_SIZE'],\n#             gradient_accumulation_steps=HYPERPARAMS['GRADIENT_ACCUMULATION'],\n#             eval_strategy=\"steps\",\n#             num_train_epochs=HYPERPARAMS['EPOCHS'],\n#             fp16=torch.cuda.is_available(),\n#             save_steps=500,\n#             eval_steps=500,\n#             logging_steps=100,\n#             learning_rate=HYPERPARAMS['LEARNING_RATE'],\n#             warmup_ratio=HYPERPARAMS['WARMUP_RATIO'],\n#             weight_decay=HYPERPARAMS['WEIGHT_DECAY'],\n#             save_total_limit=2,\n#             load_best_model_at_end=True,\n#             metric_for_best_model=\"nls\",\n#             greater_is_better=True,\n#             push_to_hub=False,\n#             report_to=[\"none\"]\n#         )\n        \n#         trainer = Trainer(\n#             model=model,\n#             data_collator=data_collator,\n#             args=training_args,\n#             compute_metrics=lambda pred: compute_metrics(pred, processor),\n#             train_dataset=train_ds,\n#             eval_dataset=val_ds,\n#             tokenizer=processor.feature_extractor,\n#         )\n        \n#         print(\"üöÄ Starting training...\")\n#         trainer.train()\n#         print(\"‚úÖ Training complete!\")\n        \n#         # Save best model AND processor\n#         best_model_path = trainer.state.best_model_checkpoint or OUTPUT_DIR\n#         print(f\"\\nüíæ Saving model to: {best_model_path}\")\n        \n#         # Save the processor to the same directory\n#         processor.save_pretrained(best_model_path)\n#         print(f\"‚úÖ Processor saved to: {best_model_path}\")\n        \n#         # 8. VALIDATION EVALUATION\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 5: VALIDATION EVALUATION (Competition Metric)\")\n#         print(\"=\"*70)\n        \n#         # Use the already loaded processor and load best model\n#         print(f\"\\n‚è≥ Loading best model from: {best_model_path}\")\n#         try:\n#             best_model = Wav2Vec2ForCTC.from_pretrained(best_model_path).to(device)\n#             best_processor = processor  # Use the original processor\n#             print(\"‚úÖ Best model loaded successfully\")\n#         except Exception as e:\n#             print(f\"‚ö†Ô∏è Could not load best checkpoint, using current model\")\n#             best_model = model\n#             best_processor = processor\n        \n#         val_predictions = []\n#         val_references = []\n        \n#         print(\"\\n‚è≥ Running validation inference...\")\n#         best_model.eval()  # Set to evaluation mode\n        \n#         for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Validation\"):\n#             try:\n#                 audio = load_audio(row['audio_path'])\n#                 inputs = best_processor(audio, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n                \n#                 with torch.no_grad():\n#                     logits = best_model(inputs.input_values.to(device)).logits\n                \n#                 pred_ids = torch.argmax(logits, dim=-1)\n#                 pred_text = best_processor.batch_decode(pred_ids)[0]\n#                 pred_text = clean_prediction(pred_text)  # Remove [UNK]\n                \n#                 val_predictions.append(pred_text)\n#                 val_references.append(row['transcript_normalized'])\n#             except Exception as e:\n#                 print(f\"\\n‚ö†Ô∏è Error processing {row['audio_path']}: {e}\")\n#                 val_predictions.append(\"\")\n#                 val_references.append(row['transcript_normalized'])\n        \n#         # Calculate Competition Metric: Normalized Levenshtein Similarity\n#         print(\"\\nüìä Calculating Competition Metric...\")\n#         print(\"=\"*70)\n#         print(\"Evaluation Metric: Normalized Levenshtein Similarity\")\n#         print(\"Formula: Similarity = 1.0 - (Levenshtein Distance / Max Length)\")\n#         print(\"Final Score = Mean Similarity across all samples\")\n#         print(\"=\"*70)\n        \n#         eval_result = evaluate_predictions(val_references, val_predictions)\n#         avg_nls = eval_result['mean_nls']\n#         individual_scores = eval_result['individual_scores']\n        \n#         # Save validation results with NLS scores\n#         val_results = val_df.copy()\n#         val_results['prediction'] = val_predictions\n#         val_results['reference'] = val_references\n#         val_results['nls_score'] = individual_scores\n#         val_results['levenshtein_distance'] = [\n#             Levenshtein.distance(ref, pred) \n#             for ref, pred in zip(val_references, val_predictions)\n#         ]\n        \n#         val_results[['audio_path', 'dialect', 'transcript', 'reference', 'prediction', \n#                      'levenshtein_distance', 'nls_score']].to_csv(\n#             'validation_results_detailed.csv', index=False\n#         )\n        \n#         print(f\"\\n‚úÖ VALIDATION RESULTS (Competition Metric):\")\n#         print(f\"   Mean Normalized Levenshtein Similarity: {avg_nls:.6f}\")\n#         print(f\"   Best possible score: 1.0\")\n#         print(f\"   Your score: {avg_nls:.6f} ({avg_nls*100:.2f}%)\")\n        \n#         # Show best and worst predictions\n#         val_results_sorted = val_results.sort_values('nls_score', ascending=False)\n        \n#         print(f\"\\nüèÜ Top 5 Best Predictions:\")\n#         print(val_results_sorted[['reference', 'prediction', 'nls_score']].head(5).to_string(index=False))\n        \n#         print(f\"\\n‚ö†Ô∏è Top 5 Worst Predictions:\")\n#         print(val_results_sorted[['reference', 'prediction', 'nls_score']].tail(5).to_string(index=False))\n        \n#         # Per-dialect performance\n#         print(f\"\\nüìä Performance by Dialect:\")\n#         dialect_perf = val_results.groupby('dialect')['nls_score'].agg(['mean', 'count', 'std'])\n#         print(dialect_perf.to_string())\n        \n#         # 9. TEST INFERENCE\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"STEP 6: TEST INFERENCE\")\n#         print(\"=\"*70)\n        \n#         test_audio_files = glob(os.path.join(TEST_AUDIO_ROOT, '**', '*.wav'), recursive=True)\n#         print(f\"\\nüìä Found {len(test_audio_files)} test files\")\n        \n#         if len(test_audio_files) == 0:\n#             print(\"‚ö†Ô∏è No test files found!\")\n#         else:\n#             test_predictions = []\n#             test_filenames = []\n            \n#             print(\"\\n‚è≥ Running test inference...\")\n#             best_model.eval()  # Ensure evaluation mode\n            \n#             for audio_path in tqdm(test_audio_files, desc=\"Test\"):\n#                 try:\n#                     audio = load_audio(audio_path)\n#                     inputs = best_processor(audio, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n                    \n#                     with torch.no_grad():\n#                         logits = best_model(inputs.input_values.to(device)).logits\n                    \n#                     pred_ids = torch.argmax(logits, dim=-1)\n#                     pred_text = best_processor.batch_decode(pred_ids)[0]\n#                     pred_text = add_dari(pred_text)  # Clean and add dari\n                    \n#                     test_predictions.append(pred_text)\n#                     test_filenames.append(os.path.basename(audio_path))\n#                 except Exception as e:\n#                     print(f\"\\n‚ö†Ô∏è Error processing {os.path.basename(audio_path)}: {e}\")\n#                     test_predictions.append(\"‡•§\")\n#                     test_filenames.append(os.path.basename(audio_path))\n            \n#             # Save submission\n#             submission_df = pd.DataFrame({\n#                 'audio': test_filenames,\n#                 'text': test_predictions\n#             })\n            \n#             submission_df.to_csv('submission.csv', index=False)\n#             print(f\"\\n‚úÖ Submission saved: {len(submission_df)} predictions\")\n#             print(f\"\\nüìù Sample predictions:\")\n#             print(submission_df.head(10))\n        \n#         # 10. SUMMARY\n#         print(\"\\n\" + \"=\"*70)\n#         print(\"‚úÖ PIPELINE COMPLETE\")\n#         print(\"=\"*70)\n        \n#         print(f\"\\nüìã Generated files:\")\n#         print(f\"   1. comprehensive_dialect_mapping.csv - 21-column word mapping\")\n#         print(f\"   2. validation_results_detailed.csv - Validation with NLS scores\")\n#         print(f\"   3. submission.csv - Test predictions (audio, text)\")\n#         print(f\"   4. {OUTPUT_DIR}/ - Fine-tuned model\")\n        \n#         print(f\"\\nüéØ Final Competition Metrics:\")\n#         print(f\"   Validation NLS Score: {avg_nls:.6f} (Higher is better, max=1.0)\")\n#         print(f\"   Score Percentage: {avg_nls*100:.2f}%\")\n#         print(f\"   Training samples: {len(train_df)}\")\n#         print(f\"   Validation samples: {len(val_df)}\")\n#         print(f\"   Test samples: {len(test_audio_files)}\")\n#         print(f\"   Dialects: {full_df['dialect'].nunique()}\")\n        \n#         print(f\"\\nüîí Competition Compliance:\")\n#         print(f\"   ‚úÖ Model: {PRIMARY_MODEL} (Open Source)\")\n#         print(f\"   ‚úÖ License: Apache 2.0\")\n#         print(f\"   ‚úÖ Inference: Local only\")\n#         print(f\"   ‚úÖ Task: Dialect ‚Üí Standard Bengali\")\n#         print(f\"   ‚úÖ Metric: Normalized Levenshtein Similarity\")\n        \n#         print(f\"\\nüìä Evaluation Formula:\")\n#         print(f\"   Similarity = 1.0 - (Levenshtein Distance / Max(Ref Length, Pred Length))\")\n#         print(f\"   Final Score = Mean of all similarities\")\n#         print(f\"   Higher scores rank higher in competition\")\n        \n#         print(f\"\\nüí° Next Steps to Improve Score:\")\n#         print(f\"   1. Analyze worst predictions in validation_results_detailed.csv\")\n#         print(f\"   2. Check comprehensive_dialect_mapping.csv for dialect patterns\")\n#         print(f\"   3. Consider increasing EPOCHS or adjusting LEARNING_RATE\")\n#         print(f\"   4. Add data augmentation (speed perturbation, noise)\")\n#         print(f\"   5. Ensemble multiple models for better predictions\")\n        \n#     except Exception as e:\n#         print(f\"\\n‚ùå PIPELINE FAILED: {str(e)}\")\n#         import traceback\n#         traceback.print_exc()\n#         raise","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install python-Levenshtein bnunicodenormalizer transformers datasets torch librosa audiomentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:42:11.077762Z","iopub.execute_input":"2025-11-11T05:42:11.077981Z"}},"outputs":[{"name":"stdout","text":"Collecting python-Levenshtein\n  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\nCollecting bnunicodenormalizer\n  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nCollecting audiomentations\n  Downloading audiomentations-0.43.1-py3-none-any.whl.metadata (11 kB)\nCollecting Levenshtein==0.27.3 (from python-Levenshtein)\n  Downloading levenshtein-0.27.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n  Downloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.2)\nCollecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n  Downloading numpy_minmax-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting numpy-rms<1,>=0.4.2 (from audiomentations)\n  Downloading numpy_rms-0.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\nCollecting python-stretch<1,>=0.3.1 (from audiomentations)\n  Downloading python_stretch-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\nCollecting numpy>=1.17 (from transformers)\n  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.5.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy>=1.17->transformers)\n  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\n  Downloading mkl_fft-1.3.14-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Downloading mkl_fft-1.3.13-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Downloading mkl_fft-1.3.11-81-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nCollecting pandas (from datasets)\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is still looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n  Downloading pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nINFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n  Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nINFO: pip is still looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n  Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n  Downloading pandas-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n  Downloading pandas-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n  Downloading pandas-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n  Downloading pandas-1.4.4.tar.gz (4.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Downloading pandas-1.4.3.tar.gz (4.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Downloading pandas-1.4.2.tar.gz (4.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Downloading pandas-1.4.1.tar.gz (4.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# Install Dependencies\n# ========================================\n\n\n# ========================================\n# Import Libraries\n# ========================================\nimport torch\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, WhisperProcessor, WhisperForConditionalGeneration\nfrom audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, VolumeControl\nfrom bnunicodenormalizer import Normalizer\nfrom tqdm import tqdm\nimport Levenshtein\nfrom sklearn.model_selection import train_test_split\n\n# ========================================\n# Text Normalization and Data Augmentation\n# ========================================\n# Normalize Bengali Text\nbnorm = Normalizer()\n\ndef normalize_bengali(text):\n    \"\"\"Standardize Bengali text\"\"\"\n    if not text:\n        return \"\"\n    words = [bnorm(word)['normalized'] for word in str(text).split()]\n    return \" \".join([w for w in words if w])\n\n# Data Augmentation for audio\naugmentation_pipeline = Compose([\n    AddGaussianNoise(min_amplitude=0.02, max_amplitude=0.1, p=0.5),\n    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n    VolumeControl(min_gain_db=-6, max_gain_db=6, p=0.5)\n])\n\ndef load_and_augment_audio(path, sr=16000):\n    \"\"\"Load and apply augmentations on audio\"\"\"\n    audio, _ = librosa.load(path, sr=sr, mono=True)\n    audio = augmentation_pipeline(samples=audio, sample_rate=sr)\n    return audio\n\n# ========================================\n# Load Models (Whisper + Wav2Vec2)\n# ========================================\n# Load Whisper model and processor\nwhisper_model_name = \"openai/whisper-large\"\nwhisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\nwhisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)\n\n# Load Wav2Vec2 model for Bengali ASR\nwav2vec2_model_name = \"arijitx/wav2vec2-xls-r-300m-bengali\"\nwav2vec2_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_model_name)\nwav2vec2_model = Wav2Vec2ForCTC.from_pretrained(wav2vec2_model_name)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nwhisper_model.to(device)\nwav2vec2_model.to(device)\n\n# ========================================\n# Ensemble Prediction (Whisper + Wav2Vec2)\n# ========================================\ndef ensemble_predict(audio, whisper_model, whisper_processor, wav2vec2_model, wav2vec2_processor):\n    \"\"\"Combine Whisper and Wav2Vec2 predictions\"\"\"\n    \n    # Whisper Prediction\n    whisper_inputs = whisper_processor(audio, return_tensors=\"pt\", sampling_rate=16000)\n    with torch.no_grad():\n        whisper_ids = whisper_model.generate(whisper_inputs.input_values.to(device))\n    whisper_pred = whisper_processor.decode(whisper_ids[0], skip_special_tokens=True)\n\n    # Wav2Vec2 Prediction\n    wav2vec2_inputs = wav2vec2_processor(audio, return_tensors=\"pt\", sampling_rate=16000)\n    with torch.no_grad():\n        logits = wav2vec2_model(wav2vec2_inputs.input_values.to(device)).logits\n    wav2vec2_pred_ids = torch.argmax(logits, dim=-1)\n    wav2vec2_pred = wav2vec2_processor.batch_decode(wav2vec2_pred_ids)[0]\n\n    # Combine both predictions (majority voting or weighted average)\n    predictions = [whisper_pred, wav2vec2_pred]\n    return max(set(predictions), key=predictions.count)  # Majority voting\n\n# ========================================\n# Load Data and Preprocess\n# ========================================\ndef load_data_from_csv(annotation_path=\"/path/to/annotations\"):\n    \"\"\"Load training data from CSV and prepare the audio paths and transcripts\"\"\"\n    csv_files = glob(f\"{annotation_path}/*.csv\")\n    data = []\n    for csv_file in csv_files:\n        dialect_name = os.path.splitext(os.path.basename(csv_file))[0]\n        df = pd.read_csv(csv_file)\n        for _, row in df.iterrows():\n            audio_file = str(row[0])\n            transcript = str(row[1])\n            data.append({'audio_path': audio_file, 'transcript': transcript, 'dialect': dialect_name})\n    return pd.DataFrame(data)\n\ndef split_data(df, train_size=0.9):\n    \"\"\"Split data into train and validation sets\"\"\"\n    train_df, val_df = train_test_split(df, train_size=train_size, stratify=df['dialect'])\n    return train_df, val_df\n\n# ========================================\n# Evaluation with WER (Word Error Rate) and Levenshtein Distance\n# ========================================\ndef calculate_wer(reference, prediction):\n    \"\"\"Word Error Rate calculation\"\"\"\n    reference = reference.split()\n    prediction = prediction.split()\n    return Levenshtein.distance(\" \".join(reference), \" \".join(prediction)) / float(len(reference))\n\ndef evaluate_predictions(refs, preds):\n    \"\"\"Evaluate predictions on the validation set using WER\"\"\"\n    wer_scores = [calculate_wer(ref, pred) for ref, pred in zip(refs, preds)]\n    avg_wer = np.mean(wer_scores)\n    return {\"wer\": avg_wer}\n\n# ========================================\n# Fine-Tuning Models (Whisper and Wav2Vec2)\n# ========================================\nfrom transformers import Trainer, TrainingArguments\n\ndef fine_tune_model(train_dataset, eval_dataset, processor, model):\n    training_args = TrainingArguments(\n        output_dir=\"./output\",\n        evaluation_strategy=\"steps\",\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=3,\n        logging_steps=100,\n        save_steps=500,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=processor,\n    )\n    trainer.train()\n    return model, processor\n\n# ========================================\n# Test Inference and Prepare Submission\n# ========================================\ndef test_inference(test_audio_files):\n    predictions = []\n    for audio_path in test_audio_files:\n        audio = load_and_augment_audio(audio_path)\n        pred_text = ensemble_predict(audio, whisper_model, whisper_processor, wav2vec2_model, wav2vec2_processor)\n        predictions.append(pred_text)\n    \n    # Prepare submission\n    submission_df = pd.DataFrame({'audio': test_audio_files, 'text': predictions})\n    submission_df.to_csv('submission.csv', index=False)\n    return submission_df\n\n# ========================================\n# Final Model Saving\n# ========================================\n# Save the fine-tuned model and processor\nwhisper_model.save_pretrained(\"./best_whisper_model\")\nwhisper_processor.save_pretrained(\"./best_whisper_model\")\n\nwav2vec2_model.save_pretrained(\"./best_wav2vec2_model\")\nwav2vec2_processor.save_pretrained(\"./best_wav2vec2_model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}