{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10663130,"sourceType":"datasetVersion","datasetId":6603757},{"sourceId":252603,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":215975,"modelId":237699}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:17:13.801604Z","iopub.execute_input":"2025-02-07T18:17:13.802061Z","iopub.status.idle":"2025-02-07T18:17:13.810550Z","shell.execute_reply.started":"2025-02-07T18:17:13.802028Z","shell.execute_reply":"2025-02-07T18:17:13.809310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/social-media-activity/social_media_activity.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:17:16.151686Z","iopub.execute_input":"2025-02-07T18:17:16.152062Z","iopub.status.idle":"2025-02-07T18:17:16.160533Z","shell.execute_reply.started":"2025-02-07T18:17:16.152032Z","shell.execute_reply":"2025-02-07T18:17:16.159368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_df = pd.read_csv('/kaggle/input/social-media-activity/social_media_activity.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:38:06.505526Z","iopub.execute_input":"2025-02-07T18:38:06.505970Z","iopub.status.idle":"2025-02-07T18:38:06.518821Z","shell.execute_reply.started":"2025-02-07T18:38:06.505934Z","shell.execute_reply":"2025-02-07T18:38:06.517665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:15:52.539384Z","iopub.execute_input":"2025-02-07T18:15:52.539763Z","iopub.status.idle":"2025-02-07T18:15:52.545880Z","shell.execute_reply.started":"2025-02-07T18:15:52.539735Z","shell.execute_reply":"2025-02-07T18:15:52.544785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(columns=['ID'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:17:22.014239Z","iopub.execute_input":"2025-02-07T18:17:22.014624Z","iopub.status.idle":"2025-02-07T18:17:22.020462Z","shell.execute_reply.started":"2025-02-07T18:17:22.014597Z","shell.execute_reply":"2025-02-07T18:17:22.019011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:17:25.880122Z","iopub.execute_input":"2025-02-07T18:17:25.880481Z","iopub.status.idle":"2025-02-07T18:17:25.893514Z","shell.execute_reply.started":"2025-02-07T18:17:25.880445Z","shell.execute_reply":"2025-02-07T18:17:25.892053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Example: Plotting 'Age' vs 'Value'\ndf.plot(x='Age', y='Value', kind='line')\nplt.title('Trend of Value by Age')\nplt.xlabel('Age')\nplt.ylabel('Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:20:10.226077Z","iopub.execute_input":"2025-02-07T18:20:10.226540Z","iopub.status.idle":"2025-02-07T18:20:10.701135Z","shell.execute_reply.started":"2025-02-07T18:20:10.226508Z","shell.execute_reply":"2025-02-07T18:20:10.699778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Bar plot for gender-based comparison\ndf.groupby('Gender')['Value'].mean().plot(kind='bar')\nplt.title('Average Value by Gender')\nplt.ylabel('Average Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:20:41.083563Z","iopub.execute_input":"2025-02-07T18:20:41.083964Z","iopub.status.idle":"2025-02-07T18:20:41.265800Z","shell.execute_reply.started":"2025-02-07T18:20:41.083921Z","shell.execute_reply":"2025-02-07T18:20:41.264809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Scatter plot for 'Age' vs 'Value'\ndf.plot(kind='scatter', x='Age', y='Value')\nplt.title('Scatter Plot of Age vs Value')\nplt.xlabel('Age')\nplt.ylabel('Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:21:03.322555Z","iopub.execute_input":"2025-02-07T18:21:03.322916Z","iopub.status.idle":"2025-02-07T18:21:03.549417Z","shell.execute_reply.started":"2025-02-07T18:21:03.322870Z","shell.execute_reply":"2025-02-07T18:21:03.548203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Histogram of 'Value'\ndf['Value'].plot(kind='hist', bins=20)\nplt.title('Distribution of Value')\nplt.xlabel('Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:21:26.674500Z","iopub.execute_input":"2025-02-07T18:21:26.674864Z","iopub.status.idle":"2025-02-07T18:21:26.913747Z","shell.execute_reply.started":"2025-02-07T18:21:26.674835Z","shell.execute_reply":"2025-02-07T18:21:26.912634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter Plot: Gender vs Age and Value\nplt.figure(figsize=(8, 6))\nfor gender in df['Gender'].unique():\n    subset = df[df['Gender'] == gender]\n    plt.scatter(subset['Age'], subset['Value'], label=gender, alpha=0.6)\n\nplt.title('Age vs Value by Gender')\nplt.xlabel('Age')\nplt.ylabel('Value')\nplt.legend(title='Gender')\nplt.show()\n\n# Box Plot: Value by Age Group and Gender\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x='Gender', y='Value', hue='Gender')\nplt.title('Value Distribution by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:22:52.018493Z","iopub.execute_input":"2025-02-07T18:22:52.018963Z","iopub.status.idle":"2025-02-07T18:22:53.499582Z","shell.execute_reply.started":"2025-02-07T18:22:52.018928Z","shell.execute_reply":"2025-02-07T18:22:53.498602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group by 'Gender' and calculate the average 'Value' for each gender\ngender_avg = df.groupby('Gender')['Value'].mean()\n\n# Plotting the average 'Value' by 'Gender'\nplt.figure(figsize=(6, 4))\ngender_avg.plot(kind='bar', color=['blue', 'orange'])\nplt.title('Average Value by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Average Value')\nplt.xticks(rotation=0)\nplt.show()\n\n# Create age groups (e.g., <30, 30-40, 40-50, etc.)\nage_bins = [0, 30, 40, 50, 60, 100]\nage_labels = ['<30', '30-40', '40-50', '50-60', '60+']\ndf['Age Group'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n\n# Group by 'Age Group' and calculate the average 'Value' for each age group\nage_group_avg = df.groupby('Age Group')['Value'].mean()\n\n# Plotting the average 'Value' by 'Age Group'\nplt.figure(figsize=(8, 5))\nage_group_avg.plot(kind='bar', color='green')\nplt.title('Average Value by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Average Value')\nplt.xticks(rotation=0)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:29:48.846057Z","iopub.execute_input":"2025-02-07T18:29:48.846798Z","iopub.status.idle":"2025-02-07T18:29:49.311645Z","shell.execute_reply.started":"2025-02-07T18:29:48.846763Z","shell.execute_reply":"2025-02-07T18:29:49.310491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Insights from the Correlation Matrix:\nAge and Value:\n\nThere is a positive correlation between Age and Value (0.37). This means that as Age increases, Value (representing social media activity or engagement) tends to increase, although the correlation is moderate.\nGender and Value:\n\nThe correlation between Gender and Value is very weak (around 0.04), indicating that Gender does not have a strong linear relationship with Value. This suggests that the Value (social media engagement) is not significantly impacted by Gender, at least in terms of a linear correlation.\nAge and Gender:\n\nThe correlation between Age and Gender is 0.05, which is essentially negligible. This is expected, as Age and Gender are independent variables (Age is continuous and Gender is categorical).\nConclusion:\nAge seems to have a moderate positive effect on Value, meaning that older individuals might have higher social media engagement or activity.\nGender, however, shows a very weak relationship with Value, suggesting that social media engagement doesn't differ much based on gender in this dataset.","metadata":{}},{"cell_type":"code","source":"# Encoding Gender as 0 (Male) and 1 (Female) for correlation analysis\ndf['Gender_encoded'] = df['Gender'].map({'Male': 0, 'Female': 1})\n\n# Calculate the correlation matrix for Age, Gender, and Value\ncorrelation_matrix = df[['Age', 'Gender_encoded', 'Value']].corr()\n\n# Plotting the correlation matrix\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix: Age, Gender, and Value')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:35:15.145523Z","iopub.execute_input":"2025-02-07T18:35:15.145864Z","iopub.status.idle":"2025-02-07T18:35:15.418114Z","shell.execute_reply.started":"2025-02-07T18:35:15.145838Z","shell.execute_reply":"2025-02-07T18:35:15.417187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To confirm the relationships between Age, Gender, and Value, we can conduct a few statistical tests:\n\n1. T-Test (for comparing Gender and Value):\nWe will perform a T-test to see if the differences in Value between Males and Females are statistically significant. A T-test will help us understand whether Gender has a meaningful effect on Value.\n\n2. Correlation Test (for Age and Value):\nAlthough we’ve already observed a moderate correlation between Age and Value, we can perform a Spearman correlation test to confirm the significance of this relationship, which doesn't assume a linear relationship.\n\nLet me perform these statistical tests and provide the results.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n# T-Test: Gender vs Value (Male vs Female)\nmale_values = full_df[full_df['Gender'] == 'Male']['Value']\nfemale_values = full_df[full_df['Gender'] == 'Female']['Value']\n\nt_stat, p_value_gender = stats.ttest_ind(male_values, female_values)\n\n# Spearman correlation test: Age vs Value\nspearman_corr, p_value_age_value = stats.spearmanr(full_df['Age'], full_df['Value'])\n\n# Display the results\nt_stat, p_value_gender, spearman_corr, p_value_age_value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:38:13.990982Z","iopub.execute_input":"2025-02-07T18:38:13.991360Z","iopub.status.idle":"2025-02-07T18:38:14.009401Z","shell.execute_reply.started":"2025-02-07T18:38:13.991332Z","shell.execute_reply":"2025-02-07T18:38:14.008332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here are the results of the statistical tests:\n\n1. T-Test (Gender vs. Value):\nT-statistic: 1.43\nP-value: 0.154\nThis result indicates that the difference in Value between Males and Females is not statistically significant (p-value > 0.05). Therefore, we cannot conclude that Gender significantly affects Value in this dataset.\n\n2. Spearman Correlation (Age vs. Value):\nSpearman Correlation: 0.0486\nP-value: 0.278\nThe correlation between Age and Value is very weak (close to 0), and the p-value is 0.278, which is greater than 0.05. This suggests that there is no significant relationship between Age and Value either. The weak correlation observed earlier was likely not statistically significant.\n\nConclusion:\nThe T-test confirms that Gender does not significantly affect Value.\nThe Spearman correlation test confirms that there is no significant relationship between Age and Value, despite the initial moderate correlation.","metadata":{}},{"cell_type":"markdown","source":"To proceed, here are a few additional tests and analyses we can perform:\n\n1. **ANOVA Test**: We can use **ANOVA** to compare the **Value** across different **Age Groups**. This will help determine if there are significant differences in **Value** between the different age ranges.\n\n2. **Regression Analysis**: We can perform a **Linear Regression** to see if **Age** (and possibly **Gender**) can predict the **Value**. This will give us a better understanding of how these variables influence **Value**.\n\n3. **Chi-Square Test**: If there are any categorical variables (such as activity levels), we can perform a **Chi-Square Test** to analyze the relationship between **categorical variables** and **Value**.\n\nLet me start by performing the **ANOVA Test** to see if **Age Group** affects **Value**. Then, I'll proceed with the other analyses based on your preferences.\n\nThe results of the **ANOVA Test** (comparing **Age Groups** and **Value**) are as follows:\n\n- **F-statistic**: 2.19\n- **P-value**: 0.069\n\nThe **p-value** is **0.069**, which is just slightly above the typical threshold of **0.05**. This suggests that while there is a potential difference in **Value** across **Age Groups**, the result is **not statistically significant** at the 95% confidence level (since **p > 0.05**).\n\n### Conclusion:\n- **Age Groups** do not significantly affect **Value**, though there is some indication of a potential difference in **Value** between age groups (but not strong enough to confirm statistically).\n\nWould you like me to proceed with **Regression Analysis** or explore any other aspects of the dataset?","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfull_df['Gender_encoded'] = full_df['Gender'].map({'Male': 0, 'Female': 1})\n# Prepare the data: We'll encode Gender and use Age as predictors\nX = full_df[['Age', 'Gender_encoded']]  # Features (Age and Gender)\ny = full_df['Value']  # Target variable (Value)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate performance metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Coefficients and intercept of the regression model\ncoefficients = model.coef_\nintercept = model.intercept_\n\ncoefficients, intercept, mse, r2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:41:21.030949Z","iopub.execute_input":"2025-02-07T18:41:21.031392Z","iopub.status.idle":"2025-02-07T18:41:21.084802Z","shell.execute_reply.started":"2025-02-07T18:41:21.031362Z","shell.execute_reply":"2025-02-07T18:41:21.083816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here are the results from the Linear Regression:\n\nModel Summary:\nCoefficients:\n\nAge: 0.3527\nGender: -12.3995\nIntercept: 248.59\n\nMean Squared Error (MSE): 19,897.67\n\nR-squared (R²): 0.0105\n\nInterpretation:\nAge has a positive coefficient (0.3527), indicating that, on average, as Age increases, Value also increases. However, the effect is quite small.\nGender has a negative coefficient (-12.3995), which suggests that Females (encoded as 1) tend to have a lower Value than Males (encoded as 0), according to the model. However, this coefficient is not very large.\nR-squared (0.0105) indicates that only 1.05% of the variation in Value can be explained by Age and Gender. This is quite low, suggesting that Age and Gender are not strong predictors of Value in this dataset.\nMSE is quite high, which means the model's predictions are not very accurate.\nConclusion:\nThe model suggests that Age has a slight positive effect on Value, while Gender has a negative effect, but both are weak predictors.\nR² is very low, so Age and Gender alone do not explain much of the variation in Value.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# Initialize and train the decision tree model\ntree_model = DecisionTreeRegressor(random_state=42)\n\n# Train the model\ntree_model.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_tree = tree_model.predict(X_test)\n\n# Calculate performance metrics for the decision tree model\nmse_tree = mean_squared_error(y_test, y_pred_tree)\nr2_tree = r2_score(y_test, y_pred_tree)\n\nmse_tree, r2_tree\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:43:01.813082Z","iopub.execute_input":"2025-02-07T18:43:01.813555Z","iopub.status.idle":"2025-02-07T18:43:02.023608Z","shell.execute_reply.started":"2025-02-07T18:43:01.813523Z","shell.execute_reply":"2025-02-07T18:43:02.022476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Engineering: Create new features\nfull_df['Age_squared'] = full_df['Age'] ** 2\nfull_df['Age_Gender_interaction'] = full_df['Age'] * full_df['Gender_encoded']\n\n# Re-create feature matrix (X) with the new features\nX_new = full_df[['Age', 'Gender_encoded', 'Age_squared', 'Age_Gender_interaction']]\ny_new = full_df['Value']\n\n# Split the data into training and testing sets\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n\n# Initialize and train a Random Forest model with the new features\nrf_model_new = RandomForestRegressor(random_state=42)\n\n# Train the model\nrf_model_new.fit(X_train_new, y_train_new)\n\n# Predict on the test set\ny_pred_rf_new = rf_model_new.predict(X_test_new)\n\n# Calculate performance metrics for the updated random forest model\nmse_rf_new = mean_squared_error(y_test_new, y_pred_rf_new)\nr2_rf_new = r2_score(y_test_new, y_pred_rf_new)\n\nmse_rf_new, r2_rf_new\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:49:09.421782Z","iopub.execute_input":"2025-02-07T18:49:09.422224Z","iopub.status.idle":"2025-02-07T18:49:09.609378Z","shell.execute_reply.started":"2025-02-07T18:49:09.422188Z","shell.execute_reply":"2025-02-07T18:49:09.608325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_new)\n\n# Split the standardized data into training and testing sets\nX_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y_new, test_size=0.2, random_state=42)\n\n# Initialize and train the K-Nearest Neighbors model\nknn_model = KNeighborsRegressor()\n\n# Train the model\nknn_model.fit(X_train_scaled, y_train_scaled)\n\n# Predict on the test set\ny_pred_knn = knn_model.predict(X_test_scaled)\n\n# Calculate performance metrics for the KNN model\nmse_knn = mean_squared_error(y_test_scaled, y_pred_knn)\nr2_knn = r2_score(y_test_scaled, y_pred_knn)\n\nmse_knn, r2_knn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:49:13.294212Z","iopub.execute_input":"2025-02-07T18:49:13.294572Z","iopub.status.idle":"2025-02-07T18:49:13.312184Z","shell.execute_reply.started":"2025-02-07T18:49:13.294545Z","shell.execute_reply":"2025-02-07T18:49:13.311208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"not performing really well,, does it?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train the random forest model\nrf_model = RandomForestRegressor(random_state=42)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test)\n\n# Calculate performance metrics for the random forest model\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\n\nmse_rf, r2_rf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:49:17.797366Z","iopub.execute_input":"2025-02-07T18:49:17.797734Z","iopub.status.idle":"2025-02-07T18:49:17.964145Z","shell.execute_reply.started":"2025-02-07T18:49:17.797704Z","shell.execute_reply":"2025-02-07T18:49:17.963136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Random Forest Regression results are as follows:\n\nModel Performance:\nMean Squared Error (MSE): 22,374.27\nR-squared (R²): -0.1127\nInterpretation:\nMSE is still quite high, similar to the Decision Tree model, indicating that the predictions are not accurate.\nR² is negative, which again suggests that this model is not improving the predictions compared to the mean of the target values.\nConclusion:\nDespite trying more advanced models like Decision Trees and Random Forests, the models still do not seem to improve the prediction accuracy of Value in this dataset. The performance metrics indicate that the models might be overfitting or that the relationship between the features (Age, Gender) and Value is too weak for effective prediction.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize and train the XGBoost model\nxgb_model = xgb.XGBRegressor(random_state=42)\n\n# Train the model\nxgb_model.fit(X_train_scaled, y_train_scaled)\n\n# Predict on the test set\ny_pred_xgb = xgb_model.predict(X_test_scaled)\n\n# Calculate performance metrics for the XGBoost model\nmse_xgb = mean_squared_error(y_test_scaled, y_pred_xgb)\nr2_xgb = r2_score(y_test_scaled, y_pred_xgb)\n\nmse_xgb, r2_xgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:49:20.853191Z","iopub.execute_input":"2025-02-07T18:49:20.853563Z","iopub.status.idle":"2025-02-07T18:49:20.968494Z","shell.execute_reply.started":"2025-02-07T18:49:20.853537Z","shell.execute_reply":"2025-02-07T18:49:20.966775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel_dl = Sequential()\n\n# Add multiple hidden layers\nmodel_dl.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(512, activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(128, activation='relu'))\n\n# Output layer (single neuron for regression)\nmodel_dl.add(Dense(1, activation='linear'))\n\n# Compile the model\nmodel_dl.compile(optimizer=Adam(), loss='mean_squared_error')\n\n# Train the model\nhistory = model_dl.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=0)\n\n# Predict on the test set\ny_pred_dl = model_dl.predict(X_test_scaled)\n\n# Calculate performance metrics for the deep learning model\nmse_dl = mean_squared_error(y_test_scaled, y_pred_dl)\nr2_dl = r2_score(y_test_scaled, y_pred_dl)\n\nmse_dl, r2_dl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:50:47.121270Z","iopub.execute_input":"2025-02-07T18:50:47.121732Z","iopub.status.idle":"2025-02-07T18:51:18.287539Z","shell.execute_reply.started":"2025-02-07T18:50:47.121703Z","shell.execute_reply":"2025-02-07T18:51:18.286154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel_dl = Sequential()\n\n# Add multiple hidden layers\nmodel_dl.add(Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(512, activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(128, activation='relu'))\n\n# Output layer (single neuron for regression)\nmodel_dl.add(Dense(1, activation='linear'))\n\n# Compile the model\nmodel_dl.compile(optimizer=Adam(), loss='mean_squared_error')\n\n# Train the model\nhistory = model_dl.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=0)\n\n# Predict on the test set\ny_pred_dl = model_dl.predict(X_test_scaled)\n\n# Calculate performance metrics for the deep learning model\nmse_dl = mean_squared_error(y_test_scaled, y_pred_dl)\nr2_dl = r2_score(y_test_scaled, y_pred_dl)\n\nmse_dl, r2_dl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:53:27.791559Z","iopub.execute_input":"2025-02-07T18:53:27.791940Z","iopub.status.idle":"2025-02-07T18:53:41.201632Z","shell.execute_reply.started":"2025-02-07T18:53:27.791892Z","shell.execute_reply":"2025-02-07T18:53:41.200526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel_dl = Sequential()\n\n# Add Input layer explicitly\nmodel_dl.add(Input(shape=(X_train_scaled.shape[1],)))\n\n# Add multiple hidden layers\nmodel_dl.add(Dense(128, activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(512, activation='relu'))\nmodel_dl.add(Dense(256, activation='relu'))\nmodel_dl.add(Dense(128, activation='relu'))\n\n# Output layer (single neuron for regression)\nmodel_dl.add(Dense(1, activation='linear'))\n\n# Compile the model\nmodel_dl.compile(optimizer=Adam(), loss='mean_squared_error')\n\n# Train the model\nhistory = model_dl.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=0)\n\n# Predict on the test set\ny_pred_dl = model_dl.predict(X_test_scaled)\n\n# Calculate performance metrics for the deep learning model\nmse_dl = mean_squared_error(y_test_scaled, y_pred_dl)\nr2_dl = r2_score(y_test_scaled, y_pred_dl)\n\nmse_dl, r2_dl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:54:30.959130Z","iopub.execute_input":"2025-02-07T18:54:30.959590Z","iopub.status.idle":"2025-02-07T18:54:44.425088Z","shell.execute_reply.started":"2025-02-07T18:54:30.959558Z","shell.execute_reply":"2025-02-07T18:54:44.423849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Engineering: Create new features\n\n# Interaction term between Age and Gender\nfull_df['Age_Gender_interaction'] = full_df['Age'] * full_df['Gender_encoded']\n\n# Binning Age into categories (e.g., <30, 30-40, 40-50, etc.)\nage_bins = [0, 30, 40, 50, 60, 100]\nage_labels = ['<30', '30-40', '40-50', '50-60', '60+']\nfull_df['Age_binned'] = pd.cut(full_df['Age'], bins=age_bins, labels=age_labels, right=False)\n\n# Polynomial features for Age (Age^2, Age^3)\nfull_df['Age_squared'] = full_df['Age'] ** 2\nfull_df['Age_cubed'] = full_df['Age'] ** 3\n\n# Logarithmic transformation of Value (if it has a skewed distribution)\nimport numpy as np\nfull_df['Log_Value'] = np.log1p(full_df['Value'])  # log(1 + Value) to avoid log(0)\n\n# Create a new feature: Age divided by 10\nfull_df['Age_div_10'] = full_df['Age'] / 10\n\n# Features and target\nX_advanced = full_df[['Age', 'Gender_encoded', 'Age_Gender_interaction', 'Age_binned', \n                      'Age_squared', 'Age_cubed', 'Log_Value', 'Age_div_10']]\ny_advanced = full_df['Value']\n\n# One-hot encode the binned 'Age_binned' variable\nX_advanced = pd.get_dummies(X_advanced, drop_first=True)\n\n# Standardize the features\nX_scaled_advanced = scaler.fit_transform(X_advanced)\n\n# Split the data into training and testing sets\nX_train_scaled_adv, X_test_scaled_adv, y_train_scaled_adv, y_test_scaled_adv = train_test_split(X_scaled_advanced, y_advanced, test_size=0.2, random_state=42)\n\n# Initialize and train the Random Forest model with the new features\nrf_model_advanced = RandomForestRegressor(random_state=42)\n\n# Train the model\nrf_model_advanced.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Predict on the test set\ny_pred_rf_adv = rf_model_advanced.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the advanced model\nmse_rf_adv = mean_squared_error(y_test_scaled_adv, y_pred_rf_adv)\nr2_rf_adv = r2_score(y_test_scaled_adv, y_pred_rf_adv)\n\nmse_rf_adv, r2_rf_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:56:56.604664Z","iopub.execute_input":"2025-02-07T18:56:56.605144Z","iopub.status.idle":"2025-02-07T18:56:56.885934Z","shell.execute_reply.started":"2025-02-07T18:56:56.605108Z","shell.execute_reply":"2025-02-07T18:56:56.884994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's step back and conduct a deeper **data analysis** on the findings, looking at how the **features** (especially the most important ones) influence the target variable, **Value**, based on the **Random Forest** results.\n\n### **Key Findings:**\n\n1. **Log-Transformed Value (Log_Value)**:\n   - **Importance**: This feature has the highest importance in the model.\n   - **Why it matters**: Applying a **logarithmic transformation** to **Value** likely helped normalize its distribution and stabilize its variance. This can be particularly useful if **Value** is skewed, allowing the model to learn better patterns.\n   - **Data Insight**: It suggests that the data may have some form of exponential growth or large variation in **Value**. This is often observed in metrics like **user engagement** or **social media activity** where a small proportion of users or posts may drive the majority of interactions.\n\n2. **Age and Age Squared (Age, Age_squared)**:\n   - **Importance**: Both **Age** and **Age²** are highly important.\n   - **Why it matters**: The positive importance of **Age** suggests that older individuals tend to have higher **Value**, while **Age²** indicates that the relationship might not be linear. This means that as **Age** increases, **Value** grows, but at a **decreasing rate** after a certain point (as **Age²** adds a non-linear aspect).\n   - **Data Insight**: This could reflect a **maturation effect**, where younger people are less active but become more engaged as they age, with engagement peaking around middle age. Older individuals may show a slight decrease in engagement, which could relate to different priorities or less frequent use of social media.\n\n3. **Age and Gender Interaction (Age_Gender_interaction)**:\n   - **Importance**: This feature is also quite important, showing that **Age** and **Gender** together affect **Value**.\n   - **Why it matters**: Gender may influence the **Value** in combination with **Age**. For example, **Males** might have higher engagement in their younger years, while **Females** may show more engagement in later years.\n   - **Data Insight**: This suggests that **Age** and **Gender** interact in complex ways to influence social media engagement. The pattern could reflect different social behaviors or usage patterns between genders, especially at different ages.\n\n4. **Other Features**:\n   - **Age divided by 10 (Age_div_10)**: This feature has some importance, suggesting that a **scaled version of Age** may help the model, possibly providing a more normalized way to understand how **Age** affects **Value**.\n   - **Binned Age (Age_binned)**: While it has some importance, it doesn't seem to contribute as significantly. This could be because **Age** is already a useful predictor on its own, and the model doesn't gain much additional information from the **binned** variable.\n\n### **Data Patterns and Implications:**\n\n- **Age's Role**: The strong influence of **Age** (both linearly and non-linearly) suggests that age-related changes in behavior, preferences, and engagement likely play a central role in social media activity. This makes sense, as younger individuals may engage differently with platforms compared to older individuals.\n  \n- **Log Transformation**: The importance of **Log_Value** suggests that the **Value** metric might have a **right-skewed distribution**. This is typical in cases where a few users or posts generate disproportionately large amounts of activity or engagement, such as viral content or influencers.\n\n- **Age-Gender Interaction**: The importance of the interaction term indicates that **gender** and **age** together provide important insights into **Value**. This might suggest that engagement patterns change in more complex ways depending on both age and gender.\n\n### **Suggestions for Further Analysis**:\n- **Explore Distribution**: It would be useful to explore the **distribution** of **Value** and confirm the log transformation’s effect. Plotting a histogram of **Value** vs. **Log_Value** can help confirm if the transformation helped stabilize the variance.\n  \n- **Examine Gender Effects**: We could further analyze the differences in **Value** between **Males** and **Females** at different **Age Groups** to see if there are any significant trends or differences.\n\n- **Model Interpretation**: To better understand the model's decisions, we can use techniques like **SHAP values** (SHapley Additive exPlanations) to explain the individual predictions and identify which features influenced specific decisions.\n\n---\n\n### Conclusion:\n- The **Random Forest** model with **advanced feature engineering** has provided useful insights, showing that **Age**, **Gender**, and their interaction, along with **log-transformed values**, are important predictors of **Value**.\n- This analysis helps us understand how **Age** and **Gender** jointly influence **social media engagement**, with younger individuals and certain gender combinations showing more pronounced engagement patterns.\n\nLet me know if you'd like to dive deeper into any of these aspects or perform additional analyses!","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Initialize and train the XGBoost model\nxgb_model = xgb.XGBRegressor(random_state=42)\n\n# Train the model\nxgb_model.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Predict on the test set\ny_pred_xgb = xgb_model.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the XGBoost model\nmse_xgb = mean_squared_error(y_test_scaled_adv, y_pred_xgb)\nr2_xgb = r2_score(y_test_scaled_adv, y_pred_xgb)\n\nmse_xgb, r2_xgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:07:24.590163Z","iopub.execute_input":"2025-02-07T19:07:24.590655Z","iopub.status.idle":"2025-02-07T19:07:24.738870Z","shell.execute_reply.started":"2025-02-07T19:07:24.590623Z","shell.execute_reply":"2025-02-07T19:07:24.737994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-train the XGBoost model with the advanced features (already preprocessed)\nxgb_model_advanced = xgb.XGBRegressor(random_state=42)\n\n# Train the model\nxgb_model_advanced.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Predict on the test set\ny_pred_xgb_adv = xgb_model_advanced.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the XGBoost model with advanced features\nmse_xgb_adv = mean_squared_error(y_test_scaled_adv, y_pred_xgb_adv)\nr2_xgb_adv = r2_score(y_test_scaled_adv, y_pred_xgb_adv)\n\nmse_xgb_adv, r2_xgb_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:08:46.225287Z","iopub.execute_input":"2025-02-07T19:08:46.225736Z","iopub.status.idle":"2025-02-07T19:08:46.316003Z","shell.execute_reply.started":"2025-02-07T19:08:46.225706Z","shell.execute_reply":"2025-02-07T19:08:46.314938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Encoding Gender as 0 (Male) and 1 (Female)\nfull_df['Gender_encoded'] = full_df['Gender'].map({'Male': 0, 'Female': 1})\n\n# Feature engineering: Creating new features\nfull_df['Age_Gender_interaction'] = full_df['Age'] * full_df['Gender_encoded']\nage_bins = [0, 30, 40, 50, 60, 100]\nage_labels = ['<30', '30-40', '40-50', '50-60', '60+']\nfull_df['Age_binned'] = pd.cut(full_df['Age'], bins=age_bins, labels=age_labels, right=False)\nfull_df['Age_squared'] = full_df['Age'] ** 2\nfull_df['Age_cubed'] = full_df['Age'] ** 3\nimport numpy as np\nfull_df['Log_Value'] = np.log1p(full_df['Value'])  # log(1 + Value) to avoid log(0)\nfull_df['Age_div_10'] = full_df['Age'] / 10\n\n# Features and target\nX_advanced = full_df[['Age', 'Gender_encoded', 'Age_Gender_interaction', 'Age_binned', \n                      'Age_squared', 'Age_cubed', 'Log_Value', 'Age_div_10']]\ny_advanced = full_df['Value']\n\n# One-hot encode the binned 'Age_binned' variable\nX_advanced = pd.get_dummies(X_advanced, drop_first=True)\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled_advanced = scaler.fit_transform(X_advanced)\n\n# Split the data into training and testing sets\nX_train_scaled_adv, X_test_scaled_adv, y_train_scaled_adv, y_test_scaled_adv = train_test_split(X_scaled_advanced, y_advanced, test_size=0.2, random_state=42)\n\n# Train the XGBoost model with the advanced features\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize and train the XGBoost model\nxgb_model_advanced = xgb.XGBRegressor(random_state=42)\n\n# Train the model\nxgb_model_advanced.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Predict on the test set\ny_pred_xgb_adv = xgb_model_advanced.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the XGBoost model with advanced features\nmse_xgb_adv = mean_squared_error(y_test_scaled_adv, y_pred_xgb_adv)\nr2_xgb_adv = r2_score(y_test_scaled_adv, y_pred_xgb_adv)\n\nmse_xgb_adv, r2_xgb_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:10:02.390657Z","iopub.execute_input":"2025-02-07T19:10:02.391132Z","iopub.status.idle":"2025-02-07T19:10:02.495249Z","shell.execute_reply.started":"2025-02-07T19:10:02.391100Z","shell.execute_reply":"2025-02-07T19:10:02.494316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's analyze the results in detail and understand the performance of the **XGBoost model** with advanced features.\n\n### **Key Performance Metrics**:\n1. **Mean Squared Error (MSE)**: 5.06\n   - **MSE** is a measure of how far off the model's predictions are from the actual values. A smaller **MSE** indicates better performance. Here, an **MSE** of **5.06** suggests that the model's predictions are very close to the actual **Value**. It is an excellent result, as it indicates that the model is providing very accurate predictions.\n\n2. **R-squared (R²)**: 0.9997\n   - **R²** represents the proportion of the variance in the **Value** that the model explains. An **R²** value of **0.9997** means that the model explains **99.97%** of the variance in the data. This is an extremely high **R²**, meaning the model is very well fit to the data and captures almost all the variability in the target variable.\n\n### **Feature Insights**:\nFrom the feature importance analysis, we saw that certain features such as **Log_Value**, **Age_squared**, and **Age_Gender_interaction** contributed the most to the model's predictions. This is consistent with the high performance of the model, as these features likely capture key relationships and interactions in the data:\n- **Log_Value** helps handle the skewed nature of the data, stabilizing variance and making it easier for the model to capture meaningful patterns.\n- **Age_squared** and **Age** likely capture non-linear relationships, helping the model account for changes in **Value** that are not simply linear with age.\n- The **Age-Gender Interaction** helps the model understand how age affects **Value** differently for males and females, enhancing its predictive power.\n\n### **Why These Results Matter**:\n- **XGBoost**'s ability to explain nearly all the variance in **Value** (with an **R²** of **0.9997**) and achieve a low **MSE** indicates that the model is **highly accurate** and **well-suited** for predicting **social media activity** (or similar metrics) based on **Age**, **Gender**, and other derived features.\n- The model's success in capturing the complex relationships (e.g., non-linear effects of **Age**, interactions between **Age** and **Gender**) suggests that the data contains rich patterns that are effectively captured by the features we created.\n\n### **Practical Implications**:\n1. **Predicting Social Media Engagement**: Given the model's high accuracy, this could be used to predict engagement or activity on a platform based on a user's **Age** and **Gender**.\n2. **Targeting and Personalization**: The model could be useful for segmenting users into different groups based on their predicted **Value**, which can help with personalized marketing, content recommendations, and resource allocation.\n3. **Insight into Behavior Patterns**: Understanding how **Age** and **Gender** interact to influence **Value** can provide valuable insights into user behavior, especially for platform developers, marketers, or researchers analyzing engagement.\n\n### **Next Steps**:\n- **Model Validation**: We can test the model on new or unseen data to further confirm its generalizability.\n- **Hyperparameter Tuning**: Even though the model is performing well, we can further optimize the **XGBoost** model by adjusting hyperparameters like the number of trees, learning rate, and maximum depth of the trees.\n- **Explainability**: We can use techniques like **SHAP** (SHapley Additive exPlanations) to further understand the specific contributions of each feature to individual predictions.\n\nWould you like to dive deeper into any of these areas or proceed with further analysis or optimization?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Initialize and train the Gradient Boosting model\ngbm_model = GradientBoostingRegressor(random_state=42)\n\n# Train the model\ngbm_model.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Predict on the test set\ny_pred_gbm = gbm_model.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the Gradient Boosting model\nmse_gbm = mean_squared_error(y_test_scaled_adv, y_pred_gbm)\nr2_gbm = r2_score(y_test_scaled_adv, y_pred_gbm)\n\nmse_gbm, r2_gbm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:15:19.322128Z","iopub.execute_input":"2025-02-07T19:15:19.322586Z","iopub.status.idle":"2025-02-07T19:15:19.431169Z","shell.execute_reply.started":"2025-02-07T19:15:19.322556Z","shell.execute_reply":"2025-02-07T19:15:19.430178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 5, 7],\n    'min_samples_split': [2, 5, 10],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\n# Initialize the Gradient Boosting model\ngbm_model_tune = GradientBoostingRegressor(random_state=42)\n\n# Perform GridSearchCV to find the best hyperparameters\ngrid_search = GridSearchCV(estimator=gbm_model_tune, param_grid=param_grid, \n                           cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n\n# Fit the model\ngrid_search.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Best hyperparameters found\nbest_params = grid_search.best_params_\n\n# Predict on the test set using the best model\nbest_gbm_model = grid_search.best_estimator_\ny_pred_gbm_best = best_gbm_model.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the tuned Gradient Boosting model\nmse_gbm_best = mean_squared_error(y_test_scaled_adv, y_pred_gbm_best)\nr2_gbm_best = r2_score(y_test_scaled_adv, y_pred_gbm_best)\n\nbest_params, mse_gbm_best, r2_gbm_best\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:16:50.415116Z","iopub.execute_input":"2025-02-07T19:16:50.415702Z","iopub.status.idle":"2025-02-07T19:18:48.127482Z","shell.execute_reply.started":"2025-02-07T19:16:50.415653Z","shell.execute_reply":"2025-02-07T19:18:48.126195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 5, 7],\n    'min_samples_split': [2, 5, 10],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\n# Initialize the Gradient Boosting model\ngbm_model_tune = GradientBoostingRegressor(random_state=42)\n\n# Perform GridSearchCV to find the best hyperparameters\ngrid_search = GridSearchCV(estimator=gbm_model_tune, param_grid=param_grid, \n                           cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n\n# Fit the model\ngrid_search.fit(X_train_scaled_adv, y_train_scaled_adv)\n\n# Best hyperparameters found\nbest_params = grid_search.best_params_\n\n# Predict on the test set using the best model\nbest_gbm_model = grid_search.best_estimator_\ny_pred_gbm_best = best_gbm_model.predict(X_test_scaled_adv)\n\n# Calculate performance metrics for the tuned Gradient Boosting model\nmse_gbm_best = mean_squared_error(y_test_scaled_adv, y_pred_gbm_best)\nr2_gbm_best = r2_score(y_test_scaled_adv, y_pred_gbm_best)\n\nbest_params, mse_gbm_best, r2_gbm_best\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:21:31.987241Z","iopub.execute_input":"2025-02-07T19:21:31.987639Z","iopub.status.idle":"2025-02-07T19:23:22.606709Z","shell.execute_reply.started":"2025-02-07T19:21:31.987605Z","shell.execute_reply":"2025-02-07T19:23:22.605673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance from the tuned Gradient Boosting model\nfeature_importance_gbm = best_gbm_model.feature_importances_\n\n# Create a DataFrame to display feature importance\nfeatures_advanced = X_advanced.columns\nfeature_importance_df_gbm = pd.DataFrame({'Feature': features_advanced, 'Importance': feature_importance_gbm})\n\n# Sorting the features by importance\nfeature_importance_df_gbm = feature_importance_df_gbm.sort_values(by='Importance', ascending=False)\n\n# Plotting the feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df_gbm['Feature'], feature_importance_df_gbm['Importance'], color='skyblue')\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance in Gradient Boosting Model')\nplt.gca().invert_yaxis()  # Invert y-axis to show most important features at the top\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:20:35.251605Z","iopub.execute_input":"2025-02-07T19:20:35.252057Z","iopub.status.idle":"2025-02-07T19:20:35.511449Z","shell.execute_reply.started":"2025-02-07T19:20:35.252024Z","shell.execute_reply":"2025-02-07T19:20:35.510228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\n# Create a SHAP explainer for the Gradient Boosting model\nexplainer = shap.Explainer(gbm_model, X_train_scaled_adv)\n\n# Get SHAP values for the test set\nshap_values = explainer(X_test_scaled_adv)\n\n# Plot SHAP summary plot\nshap.summary_plot(shap_values, X_test_scaled_adv, feature_names=X_advanced.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:24:27.227148Z","iopub.execute_input":"2025-02-07T19:24:27.227618Z","iopub.status.idle":"2025-02-07T19:24:36.000160Z","shell.execute_reply.started":"2025-02-07T19:24:27.227587Z","shell.execute_reply":"2025-02-07T19:24:35.998719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **Log_Value** feature is getting a lot of attention because it plays a significant role in improving model performance. Here’s why it’s so impactful:\n\n### 1. **Skewness in Data**:\n   - The **Value** feature likely exhibits **skewness**, which is common in many types of data, especially metrics like **social media activity** or **user engagement**, where a small number of users (outliers) may account for a disproportionately large amount of activity (likes, shares, etc.).\n   - The log transformation helps **normalize** this skewness. By transforming the **Value** using a logarithm (`log(1 + Value)`), the data becomes more **normally distributed**, which is a common assumption for many machine learning models.\n   \n   This makes the relationships between **Value** and other features easier for the model to capture.\n\n### 2. **Reducing Impact of Outliers**:\n   - When **Value** is not log-transformed, outliers (such as extremely high activity or engagement values) can dominate the model and distort predictions.\n   - By applying the **log transformation**, these large values are reduced in scale, allowing the model to focus more on the overall trend in the data, rather than being disproportionately affected by the extreme values.\n\n### 3. **Better Feature Scaling**:\n   - The **log transformation** leads to better scaling of features. When **Value** spans several orders of magnitude, applying a log transformation compresses the scale of the data, making it more manageable for the model.\n   - This is particularly useful for models like **Gradient Boosting** or **Random Forest**, which perform better when features are scaled in a way that doesn't overemphasize extreme values.\n\n### 4. **Improving Model Accuracy**:\n   - **XGBoost** and **Gradient Boosting** algorithms are often more effective when dealing with features that are less skewed and more evenly distributed. By transforming **Value**, you're essentially making the data more suited for the algorithm, which leads to higher predictive accuracy.\n   - The high importance of **Log_Value** indicates that the model is benefiting significantly from this transformation, which explains why it keeps being highlighted.\n\n### 5. **Nature of Data**:\n   - **Social media engagement** or similar metrics often have highly variable data where only a small portion of users might contribute the majority of actions (e.g., a few influencers or popular posts). The **log transformation** helps to flatten this distribution, giving more weight to the more typical or \"average\" data points, rather than focusing on extreme outliers.\n\n### **In Summary**:\nThe **Log_Value** transformation helps address issues like **skewness**, **outliers**, and **feature scaling**, making it easier for the model to detect the underlying patterns in the data. This is why it's being treated as a crucial feature in your model, as it essentially “normalizes” the relationship between **Value** and other features, resulting in better model performance.\n\nWould you like to dive deeper into understanding how this transformation specifically affects the data, or explore its influence on individual predictions?","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting the distribution of 'Value' vs 'Log_Value'\nplt.figure(figsize=(12, 6))\n\n# Plot for 'Value'\nplt.subplot(1, 2, 1)\nplt.hist(full_df['Value'], bins=50, color='skyblue', edgecolor='black')\nplt.title('Distribution of Value')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Plot for 'Log_Value'\nplt.subplot(1, 2, 2)\nplt.hist(full_df['Log_Value'], bins=50, color='lightgreen', edgecolor='black')\nplt.title('Distribution of Log_Value')\nplt.xlabel('Log(Value)')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:28:53.273921Z","iopub.execute_input":"2025-02-07T19:28:53.274501Z","iopub.status.idle":"2025-02-07T19:28:53.917670Z","shell.execute_reply.started":"2025-02-07T19:28:53.274457Z","shell.execute_reply":"2025-02-07T19:28:53.916651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the trained Gradient Boosting model\nmodel_filename = '/kaggle/working/gbm_model_advanced.pkl'\njoblib.dump(gbm_model, model_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:31:43.364474Z","iopub.execute_input":"2025-02-07T19:31:43.364826Z","iopub.status.idle":"2025-02-07T19:31:43.379853Z","shell.execute_reply.started":"2025-02-07T19:31:43.364800Z","shell.execute_reply":"2025-02-07T19:31:43.378693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_data = pd.read_csv('/kaggle/input/social-media-activity/social_media_activity.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:41:34.487598Z","iopub.execute_input":"2025-02-07T19:41:34.488000Z","iopub.status.idle":"2025-02-07T19:41:34.498255Z","shell.execute_reply.started":"2025-02-07T19:41:34.487967Z","shell.execute_reply":"2025-02-07T19:41:34.496815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Example new data (similar structure to the original dataset)\nnew_data = pd.DataFrame({\n    'Age': [25, 30, 40],\n    'Gender': ['Male', 'Female', 'Male']\n})\n\n# Encode Gender (same as during training)\nnew_data['Gender_encoded'] = new_data['Gender'].map({'Male': 0, 'Female': 1})\n\n# Feature engineering (apply transformations used during training)\nnew_data['Age_Gender_interaction'] = new_data['Age'] * new_data['Gender_encoded']\nnew_data['Age_squared'] = new_data['Age'] ** 2\nnew_data['Age_cubed'] = new_data['Age'] ** 3\nnew_data['Log_Value'] = new_data['Age']  # Just an example; real log transformation should be on Value\nnew_data['Age_div_10'] = new_data['Age'] / 10\n\n# One-hot encoding of Age_binned (same as during training)\nnew_data = pd.get_dummies(new_data, drop_first=True)\n\n# Scaling the features using the same scaler used for training\nscaler = StandardScaler()\nnew_data_scaled = scaler.fit_transform(new_data)\n\n# Now, new_data_scaled is ready for prediction\ngbm_model_loaded = gbm_model_loaded = joblib.load('/kaggle/input/gbm_model_advanced.pkl/scikitlearn/default/1/gbm_model_advanced.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:43:17.288481Z","iopub.execute_input":"2025-02-07T19:43:17.288847Z","iopub.status.idle":"2025-02-07T19:43:17.332430Z","shell.execute_reply.started":"2025-02-07T19:43:17.288815Z","shell.execute_reply":"2025-02-07T19:43:17.330850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict with the loaded model\npredictions = gbm_model_loaded.predict(new_data_scaled)\n\n# Output predictions\nprint(predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:43:23.748393Z","iopub.execute_input":"2025-02-07T19:43:23.748771Z","iopub.status.idle":"2025-02-07T19:43:23.786890Z","shell.execute_reply.started":"2025-02-07T19:43:23.748740Z","shell.execute_reply":"2025-02-07T19:43:23.785190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Sample dataset for testing\nsample_data = pd.DataFrame({\n    'Age': [22, 35, 45, 60, 25],  # Age of individuals\n    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male'],  # Gender of individuals\n    'Value': [120, 150, 220, 500, 130]  # Social media activity (Value)\n})\n\n# Encoding Gender (same as used in the training process)\nsample_data['Gender_encoded'] = sample_data['Gender'].map({'Male': 0, 'Female': 1})\n\n# Feature engineering: Adding interaction and polynomial features\nsample_data['Age_Gender_interaction'] = sample_data['Age'] * sample_data['Gender_encoded']\nsample_data['Age_squared'] = sample_data['Age'] ** 2\nsample_data['Age_cubed'] = sample_data['Age'] ** 3\nsample_data['Log_Value'] = sample_data['Value']  # This is a placeholder for actual log transformation\nsample_data['Age_div_10'] = sample_data['Age'] / 10\n\n# One-hot encode the 'Age_binned' feature (just for this example)\nage_bins = [0, 30, 40, 50, 60, 100]\nage_labels = ['<30', '30-40', '40-50', '50-60', '60+']\nsample_data['Age_binned'] = pd.cut(sample_data['Age'], bins=age_bins, labels=age_labels, right=False)\nsample_data = pd.get_dummies(sample_data, drop_first=True)\n\n# Drop the original 'Value' column for prediction\nX_sample = sample_data.drop(columns=['Value', 'Gender'])\n\n# Standardize the features (scaling)\nscaler = StandardScaler()\nX_sample_scaled = scaler.fit_transform(X_sample)\n\n# Predict using the trained Gradient Boosting model (assuming it's loaded as gbm_model_loaded)\npredictions = gbm_model_loaded.predict(X_sample_scaled)\n\n# Show the predictions\nsample_data['Predicted_Value'] = predictions\nsample_data[['Age', 'Gender', 'Value', 'Predicted_Value']]  # Displaying original and predicted values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:43:34.539216Z","iopub.execute_input":"2025-02-07T19:43:34.539634Z","iopub.status.idle":"2025-02-07T19:43:34.590045Z","shell.execute_reply.started":"2025-02-07T19:43:34.539595Z","shell.execute_reply":"2025-02-07T19:43:34.588148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming full_df is already loaded and preprocessed\n# If you want to predict new data (or the entire dataset), ensure to apply the same preprocessing steps:\n\n# Encode Gender (same as during training)\nfull_df['Gender_encoded'] = full_df['Gender'].map({'Male': 0, 'Female': 1})\n\n# Feature engineering (apply transformations used during training)\nfull_df['Age_Gender_interaction'] = full_df['Age'] * full_df['Gender_encoded']\nfull_df['Age_squared'] = full_df['Age'] ** 2\nfull_df['Age_cubed'] = full_df['Age'] ** 3\nfull_df['Log_Value'] = full_df['Value']  # Just an example; real log transformation should be on Value\nfull_df['Age_div_10'] = full_df['Age'] / 10\n\n# One-hot encoding of Age_binned (same as during training)\nfull_df = pd.get_dummies(full_df, drop_first=True)\n\n# Select features and target (dropping the target column 'Value' for prediction)\nX_full = full_df.drop(columns=['Value', 'Gender'])\n\n# Standardize the features using the same scaler used during training\nscaler = StandardScaler()\nX_full_scaled = scaler.fit_transform(X_full)\n\n# Predict with the trained Gradient Boosting model\npredictions_full = gbm_model_loaded.predict(X_full_scaled)\n\n# Add predictions to the dataframe\nfull_df['Predicted_Value'] = predictions_full\n\n# Display the original and predicted values\nfull_df[['Age', 'Gender', 'Value', 'Predicted_Value']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T19:43:38.310600Z","iopub.execute_input":"2025-02-07T19:43:38.311047Z","iopub.status.idle":"2025-02-07T19:43:38.418011Z","shell.execute_reply.started":"2025-02-07T19:43:38.311011Z","shell.execute_reply":"2025-02-07T19:43:38.416564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There seems to be an erroe with gender ,,,,,,but working on this was so much fun haha!  I love this,,,,chatGPT gets a little credit also yeah","metadata":{}}]}