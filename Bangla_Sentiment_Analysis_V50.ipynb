{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 95426,
          "databundleVersionId": 11344607,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Bangla-Sentiment-Analysis-V50",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjidh090/kaggle_colab_practise/blob/main/Bangla_Sentiment_Analysis_V50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "QOjniynL5gxo"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "aiquest_bangla_sentiment_analysis_competition_path = kagglehub.competition_download('aiquest-bangla-sentiment-analysis-competition')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Yc16hD3f5gxp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T18:23:36.277713Z",
          "iopub.execute_input": "2025-04-15T18:23:36.277911Z",
          "iopub.status.idle": "2025-04-15T18:23:37.271015Z",
          "shell.execute_reply.started": "2025-04-15T18:23:36.277892Z",
          "shell.execute_reply": "2025-04-15T18:23:37.270029Z"
        },
        "id": "NHNCDvbE5gxp",
        "outputId": "3eee13de-a8fe-416c-89f1-b66763d0d070"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/aiquest-bangla-sentiment-analysis-competition/sample_submission.csv\n/kaggle/input/aiquest-bangla-sentiment-analysis-competition/train.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(\"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/train.csv\")\n",
        "sample_sub=pd.read_csv(\"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/sample_submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T18:23:37.271787Z",
          "iopub.execute_input": "2025-04-15T18:23:37.272201Z",
          "iopub.status.idle": "2025-04-15T18:23:37.310754Z",
          "shell.execute_reply.started": "2025-04-15T18:23:37.272175Z",
          "shell.execute_reply": "2025-04-15T18:23:37.309692Z"
        },
        "id": "aZQqwnkb5gxq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define Bangla stopwords\n",
        "stop_words = set(stopwords.words('bengali'))\n",
        "\n",
        "# Step 1: Custom replace rules for company_x, See Translation, Payment Tk etc.\n",
        "def custom_replace(text):\n",
        "    # Replace specific payment text\n",
        "    text = re.sub(r\"Payment Tk.*?successful\", \"‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶™‡¶∞‡¶ø‡¶∂‡ßã‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", text)\n",
        "\n",
        "    # Replace specific loan sentence\n",
        "    text = re.sub(\n",
        "        r\"‡¶ñ‡ßÅ‡¶¨ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ü‡¶æ‡¶á‡¶Æ‡ßá company_x ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶π‡¶ú‡ßá ‡¶≤‡ßã‡¶® ‡¶™‡ßá‡¶≤‡¶æ‡¶Æ\\?See Translation\",\n",
        "        \"‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø ‡¶∏‡¶Æ‡ßü‡ßá ‡¶∏‡¶π‡¶ú‡ßá ‡¶≤‡ßã‡¶® ‡¶™‡ßá‡ßü‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ‡•§\",\n",
        "        text\n",
        "    )\n",
        "\n",
        "    # Remove |See Translation or See Translation\n",
        "    text = re.sub(r'\\|?See Translation', '', text)\n",
        "\n",
        "    # Replace company_x, company_y appropriately\n",
        "    if 'company_x' in text or 'company_y' in text:\n",
        "        if re.search(r'‡¶≤‡ßã‡¶®|‡¶∏‡ßá‡¶¨‡¶æ|‡¶®‡¶ï|‡¶Ø‡ßã‡¶ó‡¶æ‡¶Ø‡ßã‡¶ó|‡¶ï‡¶≤', text):\n",
        "            text = re.sub(r'company_\\w+', '‡¶ì‡¶á ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶®', text)\n",
        "        else:\n",
        "            text = re.sub(r'company_\\w+', '‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡ßã‡¶Æ‡ßç‡¶™‡¶æ‡¶®‡¶ø', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Main cleaning function\n",
        "def full_bangla_text_cleaner(text, remove_english=True, remove_punctuation=True, normalize=True, fix_spacing=True):\n",
        "    # Apply custom replaces first\n",
        "    text = custom_replace(text)\n",
        "\n",
        "    # Lowercase everything\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove English words/numbers\n",
        "    if remove_english:\n",
        "        text = re.sub(r'[a-zA-Z0-9_]+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    if remove_punctuation:\n",
        "        text = re.sub(r'[‡•§‡••!?.,;:‚Äú‚Äù\"\\'‚Äò‚Äô‚Äî‚Ä¶()\\[\\]{}<>@#$%^&*_+=|\\\\/~`]', '', text)\n",
        "\n",
        "\n",
        "\n",
        "    # Keep only Bangla characters\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
        "\n",
        "    # Fix spacing issues with common errors\n",
        "    if fix_spacing:\n",
        "        text = re.sub(r'(\\S+)\\s ‡ßá', r'\\1‡ßá', text)\n",
        "        text = re.sub(r'(\\S+)\\s ‡¶∞‡ßá', r'\\1‡¶∞‡ßá', text)\n",
        "\n",
        "    # Basic spelling corrections\n",
        "    common_fixes = {\n",
        "        \"‡¶∏ ‡ßÅ‡¶ñ‡ßÄ‡¶®\": \"‡¶∏‡ßÅ‡¶ñ‡ßÄ‡¶®\",\n",
        "        \"‡¶∏‡¶ø ‡¶Æ‡ßç‡¶™‡¶≤\": \"‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤\",\n",
        "        \"‡¶®‡ßá ‡¶≠‡¶ø‡¶ó‡ßá‡¶ü\": \"‡¶®‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶ü\",\n",
        "        \"‡¶∂ ‡ßÅ‡¶ß‡ßÅ\": \"‡¶∂‡ßÅ‡¶ß‡ßÅ\",\n",
        "        \"‡¶∏ ‡ßç‡¶õ‡¶ø\": \"‡¶™‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\",\n",
        "        \"‡¶ï‡ßã‡¶Æ‡ßç‡¶™ ‡¶æ‡¶®‡¶ø‡¶∞\": \"‡¶ï‡ßã‡¶Æ‡ßç‡¶™‡¶æ‡¶®‡¶ø‡¶∞\",\n",
        "        \"‡¶® ‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶ü\": \"‡¶®‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶ü\",\n",
        "        \"‡ßá ‡¶®‡¶ï\": \"‡¶è ‡¶®‡¶ï\",\n",
        "        \"‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡ßá\":\"‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡ßá\",\n",
        "        \"‡¶∏‡ßÅ‡¶¶ ‡ßç‡¶õ‡¶ø\" : \"‡¶∏‡ßÅ‡¶¶ ‡¶™‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\",\n",
        "        \"‡¶∏ ‡¶ø‡¶Æ‡ßç‡¶™‡¶≤\": \"‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤\",\n",
        "        \"‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡ßá‡¶∞\":\"‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü‡ßá‡¶∞\"\n",
        "    }\n",
        "    for wrong, right in common_fixes.items():\n",
        "        text = text.replace(wrong, right)\n",
        "\n",
        "    # Unicode normalization\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Normalize letters\n",
        "    if normalize:\n",
        "        tokens = [re.sub(r'(‡¶•‡ßç)(‡¶Ø|‡¶Ø‡¶º)', '‡¶•‡ßç‡¶Ø', word) for word in tokens]\n",
        "        tokens = [re.sub(r'(‡¶Ö‡ßç)(‡¶Ø|‡¶Ø‡¶º)', '‡¶Ö‡ßç‡¶Ø', word) for word in tokens]\n",
        "\n",
        "    # Stopword filtering\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "train[\"clean_text\"] = train[\"text\"].apply(full_bangla_text_cleaner)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T18:23:37.313026Z",
          "iopub.execute_input": "2025-04-15T18:23:37.313407Z",
          "iopub.status.idle": "2025-04-15T18:23:40.245944Z",
          "shell.execute_reply.started": "2025-04-15T18:23:37.313374Z",
          "shell.execute_reply": "2025-04-15T18:23:40.244984Z"
        },
        "id": "4XaARV0T5gxq",
        "outputId": "240deadf-4188-4a54-de18-594713fb0874"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ‚úÖ Dataset columns: 'clean_text', 'sentiment', 'id'\n",
        "X_text = train[\"clean_text\"]\n",
        "y = train[\"sentiment\"]\n",
        "ids = train[\"id\"]\n",
        "\n",
        "# ‚úÖ Label Encoding\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ‚úÖ TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    ngram_range=(1, 2),\n",
        "    max_df=0.9,\n",
        "    min_df=3\n",
        ")\n",
        "X_tfidf = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# ‚úÖ Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
        "\n",
        "# ‚úÖ Best hyperparameters (replace with real ones if available)\n",
        "best_params = {\n",
        "    \"nb_alpha\": 0.1,\n",
        "    \"rf_n_estimators\": 150,\n",
        "    \"svc_c\": 1.0\n",
        "}\n",
        "\n",
        "# ‚úÖ Cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X_text), dtype=int)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(cv.split(X_tfidf, y_encoded)):\n",
        "    print(f\"üîÅ Fold {fold+1}\")\n",
        "    X_train_fold, y_train_fold = X_tfidf[train_idx], y_encoded[train_idx]\n",
        "    X_val_fold, y_val_fold = X_tfidf[val_idx], y_encoded[val_idx]\n",
        "\n",
        "    # ‚úÖ Stacking Classifier\n",
        "    model = StackingClassifier(\n",
        "        estimators=[\n",
        "            (\"nb\", MultinomialNB(alpha=best_params[\"nb_alpha\"])),\n",
        "            (\"rf\", RandomForestClassifier(n_estimators=best_params[\"rf_n_estimators\"], class_weight=class_weight_dict, random_state=42)),\n",
        "            (\"svc\", SVC(C=best_params[\"svc_c\"], probability=True, class_weight=class_weight_dict, random_state=42))\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(max_iter=1000, class_weight=class_weight_dict),\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_fold, y_train_fold)\n",
        "    preds = model.predict(X_val_fold)\n",
        "    oof_preds[val_idx] = preds\n",
        "\n",
        "# ‚úÖ Decode predictions and true labels\n",
        "true_labels = le.inverse_transform(y_encoded)\n",
        "oof_labels = le.inverse_transform(oof_preds)\n",
        "\n",
        "# ‚úÖ Classification Report\n",
        "print(\"\\nüìä Cross-Validation Classification Report:\")\n",
        "print(classification_report(true_labels, oof_labels))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T18:23:40.246944Z",
          "iopub.execute_input": "2025-04-15T18:23:40.247363Z",
          "iopub.status.idle": "2025-04-15T18:23:53.633161Z",
          "shell.execute_reply.started": "2025-04-15T18:23:40.247314Z",
          "shell.execute_reply": "2025-04-15T18:23:53.631171Z"
        },
        "id": "S6uUAUwF5gxq",
        "outputId": "690e8a9e-54a1-44fa-e388-4d526bfacf61"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "üîÅ Fold 1\nüîÅ Fold 2\nüîÅ Fold 3\nüîÅ Fold 4\nüîÅ Fold 5\n\nüìä Cross-Validation Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.83      0.83      0.83        65\n     neutral       0.88      0.95      0.92        64\n    positive       0.91      0.83      0.87        60\n\n    accuracy                           0.87       189\n   macro avg       0.87      0.87      0.87       189\nweighted avg       0.87      0.87      0.87       189\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Save submission file\n",
        "submission = pd.DataFrame({\n",
        "  \"id\": ids,\n",
        "    \"sentiment\": oof_labels})\n",
        "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
        "print(\"‚úÖ submission.csv saved.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T18:23:53.634672Z",
          "iopub.execute_input": "2025-04-15T18:23:53.635017Z",
          "iopub.status.idle": "2025-04-15T18:23:53.657183Z",
          "shell.execute_reply.started": "2025-04-15T18:23:53.634985Z",
          "shell.execute_reply": "2025-04-15T18:23:53.656437Z"
        },
        "id": "YjX36D3E5gxr",
        "outputId": "3b586e07-e075-4020-8390-793793948c98"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ submission.csv saved.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}