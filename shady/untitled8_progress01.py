# -*- coding: utf-8 -*-
"""Untitled8_progress01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ak0jtO2w_ny8YK1ffchp_53tpfOXK3ad
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
sub = pd.read_csv('/content/sample_submission (1).csv')

train

train.isnull().sum()

train.info()

print("Missing Values in Training Data:\n", train.isnull().sum())
print("\nMissing Values in Test Data:\n", test.isnull().sum())

# Check for duplicate rows
print("\nDuplicate Rows in Training Data:", train.duplicated().sum())
print("Duplicate Rows in Test Data:", test.duplicated().sum())

# Fill categorical missing values with "Unknown"
categorical_cols = ["Brand", "Material", "Size", "Laptop Compartment", "Waterproof", "Style", "Color"]
for col in categorical_cols:
    train[col].fillna("Unknown", inplace=True)
    test[col].fillna("Unknown", inplace=True)

# Fill numerical missing values with median
train["Weight Capacity (kg)"].fillna(train["Weight Capacity (kg)"].median(), inplace=True)
test["Weight Capacity (kg)"].fillna(test["Weight Capacity (kg)"].median(), inplace=True)

# Verify if missing values are handled
print(train.isnull().sum())
print(test.isnull().sum())

# Remove duplicate rows
train.drop_duplicates(inplace=True)
test.drop_duplicates(inplace=True)

# Verify duplicates are removed
print("Duplicate Rows in Training Data:", train.duplicated().sum())
print("Duplicate Rows in Test Data:", test.duplicated().sum())

# Check data types
print("Training Data Types:\n", train.dtypes)
print("\nTest Data Types:\n", test.dtypes)

# Convert categorical columns to 'category' dtype (optional)
categorical_cols = ["Brand", "Material", "Size", "Laptop Compartment", "Waterproof", "Style", "Color"]
for col in categorical_cols:
    train[col] = train[col].astype('category')
    test[col] = test[col].astype('category')

# Ensure numerical columns are correct
numerical_cols = ["Weight Capacity (kg)", "Price"]
train[numerical_cols] = train[numerical_cols].apply(pd.to_numeric, errors='coerce')
test["Weight Capacity (kg)"] = test["Weight Capacity (kg)"].apply(pd.to_numeric, errors='coerce')

def clean_categorical_data(df):
    # Standardize capitalization (e.g., "jansport" → "Jansport")
    for col in categorical_cols:
        df[col] = df[col].str.strip().str.title()
    return df

# Apply cleaning
train = clean_categorical_data(train)
test = clean_categorical_data(test)

# Check unique values in "Brand" (example)
print("Unique Brands in Training Data:\n", train["Brand"].value_counts())

def clean_categorical_data(df):
    # Standardize capitalization (e.g., "jansport" → "Jansport")
    for col in categorical_cols:
        df[col] = df[col].str.strip().str.title()
    return df

# Apply cleaning
train = clean_categorical_data(train)
test = clean_categorical_data(test)

# Check unique values in "Brand" (example)
print("Unique Brands in Training Data:\n", train["Brand"].value_counts())

# Check summary statistics
print(train[["Price", "Weight Capacity (kg)"]].describe())

# Visualize outliers (run this in a Jupyter notebook)
import matplotlib.pyplot as plt
train["Price"].plot(kind='box')
plt.show()

# Cap outliers at 99th percentile (example)
def cap_outliers(df, column):
    upper_limit = df[column].quantile(0.99)
    df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])
    return df

train = cap_outliers(train, "Price")
train = cap_outliers(train, "Weight Capacity (kg)")
test = cap_outliers(test, "Weight Capacity (kg)")

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd

# Label Encoding for "Size" (if ordered)
size_order = ['Small', 'Medium', 'Large']
train['Size'] = train['Size'].astype('category').cat.set_categories(size_order, ordered=True).cat.codes
test['Size'] = test['Size'].astype('category').cat.set_categories(size_order, ordered=True).cat.codes

# One-Hot Encoding for other categorical features
categorical_cols = ["Brand", "Material", "Laptop Compartment", "Waterproof", "Style", "Color"]
train = pd.get_dummies(train, columns=categorical_cols, drop_first=True)
test = pd.get_dummies(test, columns=categorical_cols, drop_first=True)

# Ensure both train and test have the same columns after encoding
train, test = train.align(test, join='left', axis=1, fill_value=0)

print("Encoding complete. Here’s a preview:")
print(train.head())

train

# prompt: get the True False to numerical value.

# Assuming 'train' DataFrame is already processed as in the provided code.

# Replace True/False with 1/0 for boolean columns
boolean_cols = train.select_dtypes(include=['bool']).columns
for col in boolean_cols:
    train[col] = train[col].astype(int)

print(train.head())

# prompt: I need the correlation matrix visualization

import matplotlib.pyplot as plt
# Assuming 'train' DataFrame is already processed as in the provided code.

plt.figure(figsize=(20, 10))
sns.heatmap(train.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

def feature_engineering(df):
    # 1. Create a new feature for "Non-Polyester, Non-Nylon" Material
    df["Material_Non_Poly_Nylon"] = (df["Material_Polyester"] == 0) & (df["Material_Nylon"] == 0)
    df["Material_Non_Poly_Nylon"] = df["Material_Non_Poly_Nylon"].astype(int)

    # 2. Capture "Non-Famous" Brands
    df["Non_Famous_Brand"] = (df["Brand_Jansport"] == 0) & (df["Brand_Nike"] == 0) & \
                              (df["Brand_Puma"] == 0) & (df["Brand_Under Armour"] == 0)
    df["Non_Famous_Brand"] = df["Non_Famous_Brand"].astype(int)

    # 3. Interaction Feature: "Tote & Waterproof" Bags
    df["Waterproof_Tote"] = df["Waterproof_Yes"] * df["Style_Tote"]

    # 4. Encode Style as Price Impacting Feature
    df["Style_Impact"] = df["Style_Tote"] * 1 + df["Style_Messenger"] * -1

    # 5. Drop irrelevant features
    # df.drop(columns=["Weight Capacity (kg)"], inplace=True)

    return df
train = feature_engineering(train)
test = feature_engineering(test)

# Label Encoding for "Size" (if ordered)
size_order = ['Small', 'Medium', 'Large']
train['Size'] = train['Size'].astype('category').cat.set_categories(size_order, ordered=True).cat.codes
test['Size'] = test['Size'].astype('category').cat.set_categories(size_order, ordered=True).cat.codes

# One-Hot Encoding for other categorical features
categorical_cols = ["Brand", "Material", "Laptop Compartment", "Waterproof", "Style", "Color"]
train = pd.get_dummies(train, columns=categorical_cols, drop_first=True)
test = pd.get_dummies(test, columns=categorical_cols, drop_first=True)

# Ensure both train and test have the same columns after encoding
train, test = train.align(test, join='left', axis=1, fill_value=0)





from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Define features & target
X = train.drop(columns=['Price', 'id'])  # Dropping target and ID column
y = train['Price']

# Split into train & validation sets (80-20 split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_val)

# Evaluate the model
mae = mean_absolute_error(y_val, y_pred)
print(f"Mean Absolute Error: {mae:.2f}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error

# Define features & target
X = train.drop(columns=['Price', 'id'])  # Dropping target and ID column
y = train['Price']

# Split into train & validation sets (80-20 split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),
    "LightGBM": LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_pred)
    results[name] = mae
    print(f"{name} MAE: {mae:.2f}")

# Find the best model
best_model = min(results, key=results.get)
print(f"\nBest Model: {best_model} with MAE = {results[best_model]:.2f}")

from tqdm import tqdm

import time
from tqdm import tqdm
from lightgbm import LGBMRegressor

# Start timer
start_time = time.time()

# Re-train LightGBM on the full training set
final_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# Wrap training in tqdm for progress tracking
with tqdm(total=1, desc="Training LightGBM") as pbar:
    final_model.fit(X, y)  # Train on all available data
    pbar.update(1)  # Update progress bar

# End timer
end_time = time.time()
train_time = end_time - start_time
print(f"Training completed in {train_time:.2f} seconds.")

# Make predictions on the test set
test_preds = final_model.predict(test.drop(columns=['id']))  # Ensure 'id' is removed

# Create submission DataFrame
submission = pd.DataFrame({
    'id': test['id'],
    'Price': test_preds  # Predicted prices
})

# Save to CSV (for Kaggle submission)
submission.to_csv('submission.csv', index=False)
print("Submission file saved as submission.csv")

test

import time
from tqdm import tqdm
from lightgbm import LGBMRegressor
import pandas as pd

# Load sample submission file
sample_submission = pd.read_csv("sample_submission (1).csv")

# Ensure train and test sets have the same features
X, test = X.align(test, join='left', axis=1, fill_value=0)

# Ensure test uses correct "id" values
if 'id' not in test.columns or not test['id'].equals(sample_submission['id']):
    test['id'] = sample_submission['id']  # Force correct IDs

# Start timer
start_time = time.time()

# Re-train LightGBM on the full training set
final_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# Wrap training in tqdm for progress tracking
with tqdm(total=1, desc="Training LightGBM") as pbar:
    final_model.fit(X, y)  # Train on all available data
    pbar.update(1)  # Update progress bar

# End timer
end_time = time.time()
train_time = end_time - start_time
print(f"Training completed in {train_time:.2f} seconds.")

# Make predictions on the test set
test_preds = final_model.predict(test.drop(columns=['id']))

# Create submission DataFrame with correct IDs
submission = pd.DataFrame({
    'id': sample_submission['id'],
    'Price': test_preds  # Predicted prices
})

# Save to CSV (for Kaggle submission)
submission.to_csv('submission.csv', index=False)
print("Submission file saved as submission.csv")

# prompt: I want submission using catboost model

import pandas as pd
import numpy as np
!pip install catboost

from catboost import CatBoostRegressor

# ... (Your existing code) ...

# Define features & target
X = train.drop(columns=['Price', 'id'])
y = train['Price']

# Identify categorical features for CatBoost
categorical_features_indices = np.where(X.dtypes != np.float64)[0]

# Split into train & validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize CatBoostRegressor
model = CatBoostRegressor(iterations=1000,
                          learning_rate=0.1,
                          depth=6,
                          loss_function='MAE',
                          eval_metric='MAE',
                          random_seed=42,
                          verbose=100) # Adjust parameters as needed

# Train the model
model.fit(X_train, y_train,
          cat_features=categorical_features_indices,
          eval_set=(X_val, y_val),
          use_best_model=True)


# Make predictions on the test set
test_preds = model.predict(test.drop(columns=['id']))

# Create submission DataFrame
submission = pd.DataFrame({
    'id': test['id'],
    'Price': test_preds
})

# Save to CSV
submission.to_csv('catboost_submission.csv', index=False)
print("CatBoost submission file saved as catboost_submission.csv")

# prompt: get me the submissionn using gradientboost and adaboost

import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor

# ... (Your existing code) ...

# Define features & target
X = train.drop(columns=['Price', 'id'])
y = train['Price']

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Gradient Boosting
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train, y_train)
gb_preds = gb_model.predict(X_val)
gb_mae = mean_absolute_error(y_val, gb_preds)
print(f"Gradient Boosting MAE: {gb_mae:.2f}")

# # AdaBoost
# ada_model = AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
# ada_model.fit(X_train, y_train)
# ada_preds = ada_model.predict(X_val)
# ada_mae = mean_absolute_error(y_val, ada_preds)
# print(f"AdaBoost MAE: {ada_mae:.2f}")

# Retrain the best model (e.g., GradientBoosting if it performs better) on the full dataset
best_model = gb_model # Replace with ada_model if AdaBoost performs better
best_model.fit(X, y)

# Predictions on the test set
test_preds = best_model.predict(test.drop(columns=['id']))

# Create submission file
submission = pd.DataFrame({'id': test['id'], 'Price': test_preds})
submission.to_csv('gcaboost_submission.csv', index=False)
print("Gradient Boosting submission file saved as gradientboost_submission.csv")

# Before making predictions on the test set, align the columns of 'test' with 'X'
test = test[[col for col in X.columns if col != 'Price'] + ['id']]  # Remove 'Price' from test if present

# Now make predictions
test_preds = best_model.predict(test.drop(columns=['id']))

# prompt: I need to know which performs better XGBoost
# Random Forest
# Decision Tree
# AdaBoost
# K-Nearest Neighbors (KNN)
# Logistic Regression
# Support Vector Machine (SVM)
# Naive Bayes
# Neural Networks
# Linear Regression
# Gradient Boosting Machine (GBM)
# Convolutional Neural Networks (CNN)
# Recurrent Neural Networks (RNN)
# Bayesian Optimization
# Ensemble Methods
import time
from tqdm import tqdm
# ... (Your existing code) ...
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor


# Define features & target
X = train.drop(columns=['Price', 'id'])
y = train['Price']

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
     "Logistic Regression": LogisticRegression(),
     "Naive Bayes": GaussianNB(),
    "Gradient Boosting Machine (GBM)": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
    # Add CNN, RNN, or Bayesian Optimization (if necessary) for specific tasks
}

results = {}
for name, model in models.items():
    start_time = time.time()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_pred)
    end_time = time.time()
    training_time = end_time - start_time
    results[name] = {"mae": mae, "training_time": training_time}
    print(f"{name} MAE: {mae:.2f}, Training Time: {training_time:.2f} seconds")

# Find the best model based on MAE
best_model_mae = min(results, key=lambda k: results[k]["mae"])
print(f"\nBest Model (MAE): {best_model_mae} with MAE = {results[best_model_mae]['mae']:.2f}")

# Find the best model based on training time
best_model_time = min(results, key=lambda k: results[k]["training_time"])
print(f"\nBest Model (Training Time): {best_model_time} with Training Time = {results[best_model_time]['training_time']:.2f} seconds")

from lightgbm import LGBMRegressor

lgb_model = LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=42)
lgb_model.fit(X_train, y_train)

y_pred = lgb_model.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
print(f"LightGBM MAE: {mae:.2f}")

!pip install catboost
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(iterations=500, learning_rate=0.05, depth=6, random_seed=42, verbose=0)
cat_model.fit(X_train, y_train)

y_pred = cat_model.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
print(f"CatBoost MAE: {mae:.2f}")

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

xgb = XGBRegressor(random_state=42)

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.7, 0.8, 0.9, 1.0]
}

search = RandomizedSearchCV(xgb, param_distributions=param_grid,
                            n_iter=10, scoring='neg_mean_absolute_error',
                            cv=5, verbose=2, random_state=42, n_jobs=-1)
search.fit(X_train, y_train)

best_xgb = search.best_estimator_
y_pred = best_xgb.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
print(f"Optimized XGBoost MAE: {mae:.2f}")

from sklearn.ensemble import StackingRegressor

estimators = [
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))
]

stacked_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())
stacked_model.fit(X_train, y_train)
y_pred = stacked_model.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
print(f"Stacking Model MAE: {mae:.2f}")

import torch # GPU Check for PyTorch
print("CUDA Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU Name:", torch.cuda.get_device_name(0))
    print("GPU Count:", torch.cuda.device_count())
    print("GPU Current Device:", torch.cuda.current_device())
    print("GPU Capability: ", torch.cuda.get_device_capability(0))
    print("GPU Memory Allocated:", torch.cuda.memory_allocated())
    print("GPU Memory Cached:", torch.cuda.memory_reserved())